# -*- coding: utf-8 -*-
"""
LibraryService - æœ¬åœ°åª’ä½“åº“ç®¡ç†æœåŠ¡ (ç²¾ç®€ç‰ˆ)

åŠŸèƒ½ï¼š
- åˆ‡æ¢æ­Œæ›²æ”¶è—çŠ¶æ€
- åˆ é™¤æ­Œæ›²
- åˆ·æ–°æ­Œæ‰‹æ­Œæ›²åˆ—è¡¨
- åº”ç”¨å…ƒæ•°æ®åŒ¹é…
- é‡ç½®æ•°æ®åº“

æ³¨æ„:
- æ–‡ä»¶æ‰«æåŠŸèƒ½å·²è¿ç§»è‡³ ScanService
- å…ƒæ•°æ®è¡¥å…¨åŠŸèƒ½å·²è¿ç§»è‡³ EnrichmentService

Author: google
Updated: 2026-01-30 (æŠ€æœ¯å€ºåŠ¡æ¸…ç†)
"""
from typing import Optional
from sqlalchemy.ext.asyncio import AsyncSession
import os
import shutil
from pathlib import Path
import anyio

from app.repositories.song import SongRepository
from app.repositories.artist import ArtistRepository
from app.models.song import Song
import logging
import base64
from datetime import datetime
from app.services.metadata_service import MetadataService
from app.services.scraper import ScraperService

# å¯¼å…¥æ–°æœåŠ¡
from app.services.scan_service import ScanService
from app.services.enrichment_service import EnrichmentService

logger = logging.getLogger(__name__)

class LibraryService:
    def __init__(self):
        from app.services.music_providers import MusicAggregator
        self.aggregator = MusicAggregator()
        # æ³¨å…¥æ–°æœåŠ¡
        self.scan_service = ScanService()
        self.enrichment_service = EnrichmentService()

    async def toggle_favorite(
        self,
        song_id: int,
        db: AsyncSession = None
    ) -> Optional[dict]:
        """åˆ‡æ¢æ­Œæ›²æ”¶è—çŠ¶æ€"""
        song_repo = SongRepository(db)
        song = await song_repo.toggle_favorite(song_id)
        
        if not song:
            return None
            
        # ç§»åŠ¨æ–‡ä»¶é€»è¾‘ (åŸºäºç”¨æˆ·åé¦ˆï¼šæ”¶è—å³å½’æ¡£)
        if not song.local_path:
            return {
                "song_id": song.id,
                "is_favorite": song.is_favorite,
                "new_path": None,
                "message": "çŠ¶æ€å·²æ›´æ–° (æ— æœ¬åœ°æ–‡ä»¶)"
            }

        try:
            old_path = Path(song.local_path)
            # åªæœ‰å½“æ–‡ä»¶å­˜åœ¨æ—¶æ‰å°è¯•ç§»åŠ¨
            if await anyio.to_thread.run_sync(old_path.exists):
                if song.is_favorite:
                    # æ”¶è— -> ç§»åŠ¨åˆ° favorites
                    new_dir = Path("favorites")
                else:
                    # å–æ¶ˆæ”¶è— -> ç§»åŠ¨å› audio_cache
                    new_dir = Path("audio_cache")
                
                new_dir.mkdir(exist_ok=True)
                new_path = new_dir / old_path.name
                
                # å¦‚æœç›®æ ‡å·²å­˜åœ¨æ–‡ä»¶ï¼Œå…ˆè¦†ç›–æˆ–è·³è¿‡? è¿™é‡Œé€‰æ‹©è¦†ç›–ï¼Œä¿è¯æœ€æ–°
                if new_path != old_path:
                    logger.info(f"Moving file for favorite toggle: {old_path} -> {new_path}")
                    await anyio.to_thread.run_sync(shutil.move, str(old_path), str(new_path))
                    
                    # æ›´æ–°æ•°æ®åº“è·¯å¾„
                    song.local_path = str(new_path)
                    await db.commit()
                    await db.refresh(song)
            else:
                logger.warning(f"File not found for toggle favorite: {old_path}")
                
        except Exception as e:
            logger.error(f"Failed to move file during favorite toggle: {e}")
            # å³ä½¿ç§»åŠ¨å¤±è´¥ï¼Œæ”¶è—çŠ¶æ€å·²å˜æ›´ï¼Œä¸é˜»æ–­æµç¨‹
        
        return {
            "song_id": song.id,
            "is_favorite": song.is_favorite,
            "new_path": song.local_path
        }

    async def delete_song(
        self,
        song_id: int,
        db: AsyncSession = None
    ) -> bool:
        """åˆ é™¤æ­Œæ›²"""
        song_repo = SongRepository(db)
        song = await song_repo.get(song_id)
        
        if not song:
            return False
            
        # åˆ é™¤æ–‡ä»¶
        if song.local_path:
            exists = await anyio.to_thread.run_sync(os.path.exists, song.local_path)
            if exists:
                await anyio.to_thread.run_sync(os.remove, song.local_path)
            
        # ä»æ•°æ®åº“åˆ é™¤
        success = await song_repo.delete(song_id)
        return success

    async def scan_local_files(self, db: AsyncSession):
        """
        æ‰«ææœ¬åœ°ç›®å½• (audio_cache, favorites)
        å‘ç°æœªå…¥åº“çš„æ­Œæ›²å¹¶æ·»åŠ åˆ°æ•°æ®åº“ (Song + SongSource)
        åŒæ—¶æ¸…ç†æ•°æ®åº“ä¸­ç‰©ç†æ–‡ä»¶å·²ä¸å­˜åœ¨çš„â€œæ­»é”®â€
        """
        import logging
        from datetime import datetime
        from mutagen import File as MutagenFile
        from mutagen.flac import FLAC
        from app.models.artist import Artist, ArtistSource
        from app.models.song import Song, SongSource
        from sqlalchemy import select
        
        logger = logging.getLogger(__name__)

        dirs_to_scan = ["audio_cache", "favorites"]
        new_count = 0
        removed_count = 0
        
        song_repo = SongRepository(db)
        
        # --- é˜¶æ®µ 1: æ¸…ç†é˜¶æ®µ (Pruning) ---
        # æŸ¥æ‰¾æ‰€æœ‰æ ‡è®°ä¸ºæœ¬åœ°å·²ä¸‹è½½çš„æ­Œæ›²
        stmt = select(Song).where(Song.local_path.isnot(None))
        res = await db.execute(stmt)
        all_local_songs = res.scalars().all()
        
        for song in all_local_songs:
            # æ ¡éªŒç‰©ç†æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            exists = await anyio.to_thread.run_sync(os.path.exists, song.local_path)
            if not exists:
                logger.info(f"ğŸ—‘ï¸ å‘ç°å¤±æ•ˆæœ¬åœ°æ–‡ä»¶è®°å½•ï¼Œæ­£åœ¨æ¸…ç†: {song.title} ({song.local_path})")
                
                # 1. ç§»é™¤æœ¬åœ°æºä¿¡æ¯
                from sqlalchemy import delete
                source_del_stmt = delete(SongSource).where(
                    SongSource.song_id == song.id,
                    SongSource.source == "local"
                )
                await db.execute(source_del_stmt)
                
                # 2. æ£€æŸ¥æ˜¯å¦è¿˜æœ‰å…¶ä»–åœ¨çº¿æºã€‚å¦‚æœæ²¡æœ‰ï¼Œåˆ™å½»åº•åˆ é™¤è¯¥æ­Œæ›²
                source_count_stmt = select(SongSource).where(SongSource.song_id == song.id)
                sources = (await db.execute(source_count_stmt)).scalars().all()
                
                if not sources:
                    # å½»åº•å­¤ç«‹çš„æ­Œæ›²è®°å½•ï¼Œç›´æ¥åˆ é™¤
                    await db.delete(song)
                else:
                    # ä»ç„¶æœ‰åœ¨çº¿ç›‘æ§ï¼Œåªæ˜¯æœ¬åœ°æ–‡ä»¶ä¸¢äº†ï¼Œé‡ç½®çŠ¶æ€
                    song.local_path = None
                    song.status = "PENDING"
                
                removed_count += 1
        
        await db.flush() # ç¡®ä¿æ¸…ç†ç»“æœç”Ÿæ•ˆ
        
        # --- é˜¶æ®µ 2: æ‰«æé˜¶æ®µ (Scanning) ---
        for dir_name in dirs_to_scan:
            exists = await anyio.to_thread.run_sync(os.path.exists, dir_name)
            if not exists:
                continue
                
            files = await anyio.to_thread.run_sync(os.listdir, dir_name)
            for filename in files:
                if not filename.endswith(('.mp3', '.flac', '.m4a', '.wav')):
                    continue
                    
                file_path = os.path.join(dir_name, filename).replace("\\", "/")
                
                # Check if already exists via SongSource
                stmt = select(SongSource).where(
                    SongSource.source == "local", 
                    SongSource.source_id == filename
                )
                if (await db.execute(stmt)).scalar_one_or_none():
                    logger.debug(f"Song already exists: {filename}")
                    continue

                # New file found!
                logger.info(f"ğŸ“‚ å‘ç°æ–°æœ¬åœ°æ–‡ä»¶: {file_path}")
                
                # è¯»å–æœ¬åœ°å…ƒæ•°æ®
                title = None
                artist_name = "Unknown"
                album = None
                has_metadata = False
                
                try:
                    audio_file = MutagenFile(file_path, easy=False)
                    if audio_file is not None:
                        # Extract basic info
                        if 'title' in audio_file: title = audio_file['title'][0]
                        if 'artist' in audio_file: artist_name = audio_file['artist'][0]
                        if 'album' in audio_file: album = audio_file['album'][0]
                        
                        # Extract Date
                        date_str = None
                        p_time = None
                        if 'date' in audio_file: date_str = audio_file['date'][0]
                        elif 'TDRC' in audio_file: date_str = str(audio_file['TDRC'])
                        elif 'TYER' in audio_file: date_str = str(audio_file['TYER'])
                        
                        if date_str:
                             try:
                                 # Take first 4 digits
                                 year_str = str(date_str)[:4]
                                 if year_str.isdigit():
                                     p_time = datetime.strptime(year_str, "%Y")
                             except: pass
                except Exception as e:
                    logger.warning(f"âŒ è¯»å–æ ‡ç­¾å¤±è´¥ {filename}: {e}")
                    pass
                
                # Filename Fallback
                clean_name = os.path.splitext(filename)[0]
                if not title:
                    if " - " in clean_name:
                        parts = clean_name.split(" - ", 1)
                        artist_name = parts[0].strip()
                        title = parts[1].strip()
                    else:
                        title = clean_name.strip()
                
                # Find/Create Logical Artist
                artist_repo = ArtistRepository(db)
                artist_obj = await artist_repo.get_or_create_by_name(artist_name)
                
                # Find/Create Logical Song
                # Match by Title + Artist (Enhanced fuzzy match)
                song_obj = await song_repo.get_by_title_artist(title, artist_obj.id)
                
                if not song_obj:
                    # å°è¯•å½’ä¸€åŒ–åŒ¹é… (è§£å†³ "Title (Live)" vs "Titleï¼ˆLiveï¼‰")
                    # è·å–è¯¥æ­Œæ‰‹æ‰€æœ‰æ­Œæ›²è¿›è¡Œå†…å­˜æ¯”å¯¹ (é€šå¸¸æ­Œæ‰‹æ­Œæ›²æ•°æœ‰é™ï¼Œä¸ä¼šå¤ªæ…¢)
                    all_artist_songs = await song_repo.get_by_artist(artist_obj.id)
                    norm_local_title = self._normalize_cn_brackets(title).lower().strip()
                    
                    for existing in all_artist_songs:
                         norm_db_title = self._normalize_cn_brackets(existing.title).lower().strip()
                         if norm_local_title == norm_db_title:
                             song_obj = existing
                             logger.info(f"  ğŸ”— æ¨¡ç³ŠåŒ¹é…æˆåŠŸ: '{title}' -> '{existing.title}'")
                             break
                
                if not song_obj:
                    song_obj = Song(
                        title=title,
                        album=album,
                        artist_id=artist_obj.id,
                        status="DOWNLOADED", # It's local
                        local_path=file_path,
                        created_at=datetime.now(),
                        publish_time=p_time
                    )
                    db.add(song_obj)
                    await db.flush() # Get ID
                else:
                    # Update existing record: set local_path and status
                    if not song_obj.local_path:
                        song_obj.local_path = file_path
                    song_obj.status = "DOWNLOADED"
                    if not song_obj.album and album:
                        song_obj.album = album
                
                # Check if this precise source already exists
                src_stmt = select(SongSource).where(
                    SongSource.song_id == song_obj.id,
                    SongSource.source == "local"
                )
                if not (await db.execute(src_stmt)).scalar_one_or_none():
                    # Create SongSource
                    new_source = SongSource(
                        song_id=song_obj.id,
                        source="local",
                        source_id=filename,
                        url=file_path
                    )
                    db.add(new_source)
                
                await db.commit()
                new_count += 1
                logger.info(f"  ğŸ“ å·²æ·»åŠ : {title} (æœ¬åœ°æº)")
                
        return {
            "new_files_found": new_count,
            "removed_files_count": removed_count
        }

    async def enrich_local_files(self, db: AsyncSession, limit: int = 5):
        """
        è¡¥å…¨æœ¬åœ°æ–‡ä»¶æ ‡ç­¾ (Local Tag Enrichment)
        
        ç›®æ ‡: ä¿®å¤æœ¬åœ°éŸ³é¢‘æ–‡ä»¶çš„ ID3/FLAC æ ‡ç­¾ã€‚
        é€»è¾‘:
        1. æŸ¥æ‰¾æºä¸º 'local' ä¸” (æ— å°é¢ æˆ– æ— ä¸“è¾‘ æˆ– çŠ¶æ€ä¸ºPENDING_METADATA) çš„æ­Œæ›²ã€‚
        2. å°è¯•è¯»å–ç°æœ‰ Tagã€‚
        3. å¦‚æœç¼ºå¤±å…³é”®ä¿¡æ¯ï¼Œåœ¨çº¿æœç´¢å…ƒæ•°æ®ã€‚
        4. å°†å…ƒæ•°æ®(å°é¢ã€æ­Œè¯ã€ä¸“è¾‘) **å†™å…¥** åˆ°æœ¬åœ°éŸ³é¢‘æ–‡ä»¶ã€‚
        5. åŒæ­¥æ›´æ–°æ•°æ®åº“ã€‚
        """
        import logging
        from mutagen import File as MutagenFile
        from app.models.artist import Artist
        from sqlalchemy import select, or_, and_
        from sqlalchemy.orm import selectinload
        
        logger = logging.getLogger(__name__)
        song_repo = SongRepository(db)
        metadata_service = MetadataService()
        
        # æŸ¥æ‰¾éœ€è¦ä¿®å¤æœ¬åœ°æ ‡ç­¾çš„æ­Œæ›²
        stmt = (
            select(Song)
            .options(selectinload(Song.artist))
            .where(
                and_(
                    Song.local_path.isnot(None), # V2: Local files have local_path
                    Song.status == "PENDING_METADATA" # ä¸¥æ ¼éµå¾ªçŠ¶æ€æœºï¼Œé˜²æ­¢å› æœä¸åˆ°å°é¢è€Œæ­»å¾ªç¯
                )
            )
            .limit(limit)
        )
        result = await db.execute(stmt)
        pending_songs = result.scalars().all()
        
        processed_count = 0
        processed = 0
        
        for song in pending_songs:
            processed += 1
            try:
                # Store attributes in local variables immediately to avoid MissingGreenlet later
                song_id = song.id
                song_title = song.title
                song_artist_name = song.artist.name if song.artist else ""
                song_album = song.album
                song_local_path = song.local_path
                
                if not song_local_path or not await anyio.to_thread.run_sync(os.path.exists, song_local_path):
                    logger.warning(f"Local file not found: {song_local_path}")
                    song.status = "DOWNLOADED" # é˜²æ­¢æ­»å¾ªç¯
                    continue

                logger.info(f"Enriching local tags for: {song_title}")
                
                title = song_title
                artist_name = song_artist_name
                album = song_album
                target_album = album # Initialize
                
                # 1. æ£€æµ‹æœ¬åœ°æ–‡ä»¶ç°æœ‰æ ‡ç­¾
                has_tag_title = False
                has_tag_artist = False
                has_tag_album = False
                has_tag_cover = False
                
                try:
                    from mutagen import File as MutagenFile
                    from mutagen.flac import FLAC
                    audio_check = MutagenFile(song.local_path, easy=False)
                    if audio_check:
                         # æ£€æŸ¥æ–‡æœ¬æ ‡ç­¾
                         if 'title' in audio_check or 'TIT2' in audio_check: has_tag_title = True
                         if 'artist' in audio_check or 'TPE1' in audio_check: has_tag_artist = True
                         if 'album' in audio_check or 'TALB' in audio_check: has_tag_album = True
                         
                         # æ£€æŸ¥å°é¢
                         if isinstance(audio_check, FLAC) and audio_check.pictures:
                             has_tag_cover = True
                         elif 'APIC' in audio_check or any(k.startswith('APIC:') for k in audio_check.keys()):
                             has_tag_cover = True
                except Exception as e:
                    logger.debug(f"Tag check failed: {e}")

                # 2. å†³å®šæ˜¯å¦éœ€è¦æœç´¢
                # å¦‚æœæ•°æ®åº“é‡Œæœ‰æ•°æ®ï¼Œä¸”æœ¬åœ°æ–‡ä»¶ä¹Ÿæœ‰æ•°æ®ï¼Œå°±æ²¡å¿…è¦æœç´¢äº† (é™¤éä¸ºäº†å°é¢URL)
                # ç”¨æˆ·ç—›ç‚¹: "æ–‡ä»¶å·²æœ‰ä¿¡æ¯ï¼Œä¸ä»…æœäº†è¿˜å› ä¸ºæœä¸åˆ°è€Œå¡ä½/è¦†ç›–"
                
                db_complete = (song.title and song.artist and song.album and song.cover)
                file_complete = (has_tag_title and has_tag_artist and has_tag_album and has_tag_cover)
                
                if file_complete and not song.cover:
                     # ç‰¹æ®Šæƒ…å†µ: æ–‡ä»¶å®Œç¾ï¼Œä½†DBç¼ºå°é¢URLã€‚
                     # æˆ‘ä»¬æ— æ³•ä»æ–‡ä»¶ç”ŸæˆURLç»™å‰ç«¯ã€‚å¿…é¡»æœä¸€ä¸ªURLã€‚
                     # ä½†æˆ‘ä»¬æ‰¿è¯º "ä¸ä¿®æ”¹æ–‡ä»¶"ã€‚
                     pass
                
                if file_complete and db_complete:
                     logger.info("  âœ… æ ‡ç­¾ä¸å…ƒæ•°æ®å‡å®Œæ•´ï¼Œè·³è¿‡å¤„ç†")
                     song.status = "DOWNLOADED"
                     processed_count += 1
                     continue

                # 3. åœ¨çº¿æœç´¢å…ƒæ•°æ®
                meta_result = None
                # Check if we need search: missing cover, album, OR publish_time
                search_needed = not (song.cover and song.album and song.publish_time)
                
                if search_needed:
                    logger.info(f"  ğŸ” å…ƒæ•°æ®ç¼ºå¤± (Cover: {bool(song.cover)}, Album: {bool(song.album)}, Date: {bool(song.publish_time)}), å¼€å§‹åœ¨çº¿æœç´¢...")
                    meta_result = await metadata_service.fetch_metadata(title, artist_name)
                    
                if not meta_result:
                     logger.warning("  âš ï¸ æ— æ³•è·å–å…ƒæ•°æ®ï¼Œè·³è¿‡")
                     song.status = "DOWNLOADED" # é¿å…åå¤é‡è¯•
                     continue
                     
                # 4. æ™ºèƒ½å†™å…¥ (Smart Write-Back)
                # åªæœ‰å½“æœ¬åœ°ç¼ºå¤±æ—¶æ‰å†™å…¥!
                     song.album = meta_result.album
                
                # å†™å…¥æ­Œè¯ (å¦‚æœæœ¬åœ°æ²¡æœ‰USLT? ç›®å‰ä¸æ£€æµ‹æ­Œè¯æ ‡ç­¾ï¼Œå‡è®¾ç”¨æˆ·ä¸ä»‹æ„è¦†ç›–æ­Œè¯ï¼Œæˆ–è€…é»˜è®¤è¦†ç›–)
                # ä¸ºäº†å®‰å…¨ï¼Œæš‚ä¸è¦†ç›–æ­Œè¯ï¼Œé™¤éä¸ºäº†åŠŸèƒ½ã€‚ç”±äºæ£€æŸ¥æ­Œè¯Tagæ¯”è¾ƒéº»çƒ¦ï¼Œå…ˆç•¥è¿‡ã€‚
                if meta_result and meta_result.lyrics:
                     await self._embed_lyrics_to_file(song.local_path, meta_result.lyrics)
                     
                # å†™å…¥å‘å¸ƒæ—¶é—´ (ä»…å½“æœ¬åœ°ç¼ºå¤±? æš‚æ—¶å¼ºåˆ¶æ›´æ–°ç¡®ä¿æ’åº)
                if meta_result and meta_result.publish_time:
                    await self._embed_publish_time_to_file(song.local_path, meta_result.publish_time)
                    # åŒæ­¥åˆ°æ•°æ®åº“
                    try:
                        pt_str = meta_result.publish_time
                        if len(pt_str) == 4:
                            song.publish_time = datetime.strptime(pt_str, "%Y")
                        elif len(pt_str) >= 10:
                            song.publish_time = datetime.strptime(pt_str[:10], "%Y-%m-%d")
                    except: pass

                # æ›´æ–° DB çŠ¶æ€
                song.album = target_album
                song.status = "DOWNLOADED"
                processed_count += 1
                
            except Exception as e:
                logger.error(f"Local enrichment failed for song ID {song_id if 'song_id' in locals() else 'unknown'}: {e}")
                await db.rollback()
                # Need to refresh or break, because rollback expires all objects in session
                break 
        
        await db.commit()
        await metadata_service.close()
        return processed_count

    async def refresh_library_metadata(self, db: AsyncSession, limit: int = 50):
        """
        åˆ·æ–°èµ„æ–™åº“å…ƒæ•°æ® (Library Metadata Refresh)
        
        ç›®æ ‡: ä¿®å¤æ•°æ®åº“ä¸­çš„æ˜¾ç¤ºä¿¡æ¯ (å°é¢ã€ä¸“è¾‘å)ã€‚
        æ³¨æ„: æ­¤æ“ä½œ **ä¸ä¿®æ”¹** æœ¬åœ°æ–‡ä»¶ï¼Œä»…æ›´æ–°æ•°æ®åº“å­—æ®µã€‚
        é€‚ç”¨: ä¿®æ­£ç•Œé¢ä¸Šæ˜¾ç¤ºçš„ç©ºç™½å°é¢æˆ–é”™è¯¯ä¿¡æ¯ã€‚
        """
        import logging
        from sqlalchemy import select, or_, and_
        from sqlalchemy.orm import selectinload
        
        logger = logging.getLogger(__name__)
        metadata_service = MetadataService()
        
        # æŸ¥æ‰¾ç¼ºå¤±å°é¢æˆ–ä¸“è¾‘çš„æ­Œæ›² (ä¸é™æ¥æº)
        stmt = (
            select(Song)
            .options(selectinload(Song.artist))
            .where(
                or_(
                    Song.cover == None, 
                    Song.cover == "",
                    Song.album == None,
                    Song.album == ""
                )
            )
            .limit(limit)
        )
        result = await db.execute(stmt)
        songs = result.scalars().all()
        
        updated_count = 0
        
        for song in songs:
            try:
                artist_name = song.artist.name if song.artist else ""
                if not song.title or not artist_name:
                    continue
                    
                logger.info(f"Refreshing library metadata for: {song.title}")
                
                # ä»…è·å– URL (è½»é‡çº§)
                # MetadataService fetch_metadata ä¼šä¸‹è½½å›¾ç‰‡ï¼Œæˆ‘ä»¬å¯ä»¥å¤ç”¨å®ƒï¼Œæˆ–è€…å¢åŠ ä¸€ä¸ªåªè·å–ä¿¡æ¯çš„è½»é‡æ–¹æ³•
                # è¿™é‡Œå¤ç”¨ fetch_metadata ä½†ä¸ä½¿ç”¨ cover_data
                meta_result = await metadata_service.fetch_metadata(song.title, artist_name)
                
                changed = False
                if meta_result.success:
                    if meta_result.album and not song.album:
                        song.album = meta_result.album
                        changed = True
                    
                    if meta_result.cover_url and not song.cover:
                        song.cover = meta_result.cover_url
                        changed = True
                        
                    if meta_result.publish_time and not song.publish_time:
                        from datetime import datetime
                        try:
                            pt_str = meta_result.publish_time
                            if len(pt_str) == 4:
                                song.publish_time = datetime.strptime(pt_str, "%Y")
                                changed = True
                            elif len(pt_str) >= 10:
                                song.publish_time = datetime.strptime(pt_str[:10], "%Y-%m-%d")
                                changed = True
                        except ValueError:
                            pass

                    if changed:
                        updated_count += 1
                        # æ ‡è®°å¤„ç†è¿‡? DBæ²¡æœ‰ extra å­—æ®µã€‚ 
                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å°é¢ï¼Œä¸‹æ¬¡è¿˜ä¼šæŸ¥ã€‚ä¸ºäº†é¿å…æ­»å¾ªç¯ï¼Œå‰ç«¯/è°ƒç”¨è€…åº”è¯¥æ§åˆ¶é¢‘ç‡ï¼Œæˆ–è€… we accept repeated checks.
                        # æˆ–è€…æˆ‘ä»¬å¯ä»¥ä¸´æ—¶åœ¨å†…å­˜è®°å½•? 
                        # æ— è®ºå¦‚ä½•ï¼Œcommitã€‚
                        await db.commit()
                
            except Exception as e:
                logger.error(f"Library refresh failed for {song.id}: {e}")
        
        await metadata_service.close()
        return updated_count
    
    async def _embed_cover_data(self, file_path: str, cover_data: bytes) -> bool:
        """
        å°†å°é¢æ•°æ®å†…åµŒåˆ°éŸ³é¢‘æ–‡ä»¶
        æ”¯æŒæ ¼å¼: FLAC, MP3
        """
        import logging
        from mutagen.flac import FLAC, Picture
        from mutagen.id3 import ID3, APIC
        from mutagen import File as MutagenFile
        
        logger = logging.getLogger(__name__)
        
        if not cover_data or len(cover_data) < 1000:
            logger.warning(f"  å°é¢æ•°æ®æ— æ•ˆæˆ–è¿‡å°: {len(cover_data) if cover_data else 0} å­—èŠ‚")
            return False
            
        try:
            # 1. æ£€æµ‹æ–‡ä»¶æ ¼å¼
            audio = MutagenFile(file_path, easy=False)
            
            # 2. æ ¹æ®æ ¼å¼å†…åµŒå°é¢
            if isinstance(audio, FLAC):
                # FLACæ ¼å¼
                logger.debug(f"  å†…åµŒå°é¢åˆ°FLACæ–‡ä»¶")
                picture = Picture()
                picture.type = 3  # å°é¢(front cover)
                picture.mime = "image/jpeg"
                picture.data = cover_data
                picture.desc = "Cover"
                
                # æ¸…é™¤æ—§å°é¢å¹¶æ·»åŠ æ–°å°é¢
                audio.clear_pictures()
                audio.add_picture(picture)
                audio.save()
                
                logger.info(f"  âœ… FLACå°é¢å†…åµŒæˆåŠŸ")
                return True
                
            elif hasattr(audio, 'tags') and hasattr(audio.tags, 'getall'):
                # MP3æ ¼å¼ (ID3)
                logger.debug(f"  å†…åµŒå°é¢åˆ°MP3æ–‡ä»¶")
                
                # ç¡®ä¿æœ‰ID3æ ‡ç­¾
                if audio.tags is None:
                    audio.add_tags()
                
                # æ·»åŠ APICå¸§
                audio.tags['APIC'] = APIC(
                    encoding=3,  # UTF-8
                    mime='image/jpeg',
                    type=3,  # å°é¢
                    desc='Cover',
                    data=cover_data
                )
                audio.save()
                
                logger.info(f"  âœ… MP3å°é¢å†…åµŒæˆåŠŸ")
                return True
            else:
                logger.warning(f"  ä¸æ”¯æŒçš„éŸ³é¢‘æ ¼å¼: {type(audio).__name__}")
                return False
                
        except Exception as e:
            logger.error(f"  å°é¢å†…åµŒå¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            return False
    


    async def _embed_lyrics_to_file(self, file_path: str, lyrics: str) -> bool:
        """
        å°†æ­Œè¯å†…åµŒåˆ°éŸ³é¢‘æ–‡ä»¶(FLAC/MP3)
        """
        logger.info(f"  ğŸ“ å†…åµŒæ­Œè¯åˆ°æ–‡ä»¶: {file_path}")
        
        try:
            file_ext = file_path.lower().split('.')[-1]
            
            if file_ext == 'flac':
                # FLAC: ä½¿ç”¨mutagençš„FLACç±»
                from mutagen.flac import FLAC
                audio = FLAC(file_path)
                audio['LYRICS'] = lyrics
                audio.save()
                logger.info(f"  âœ… FLACæ­Œè¯å†…åµŒæˆåŠŸ")
                return True
                
            elif file_ext == 'mp3':
                # MP3: ä½¿ç”¨ID3çš„USLTå¸§(Unsynchronized Lyrics)
                from mutagen.id3 import ID3, USLT
                try:
                    audio = ID3(file_path)
                except:
                    audio = ID3()
                
                # åˆ é™¤æ—§çš„æ­Œè¯(å¦‚æœæœ‰)
                audio.delall('USLT')
                # æ·»åŠ æ–°æ­Œè¯
                audio.add(USLT(encoding=3, lang='chi', desc='', text=lyrics))
                audio.save(file_path)
                logger.info(f"  âœ… MP3æ­Œè¯å†…åµŒæˆåŠŸ")
                return True
            else:
                logger.warning(f"  ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}")
                return False
                
        except Exception as e:
            logger.error(f"  æ­Œè¯å†…åµŒå¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            return False

    def _parse_date(self, d_str: str):
        """é€šç”¨æ—¥æœŸè§£æåŠ©æ‰‹"""
        if not d_str or not str(d_str).strip(): return None
        s = str(d_str).strip()
        
        # æ‹¦æˆªæ— æ•ˆçš„å ä½æ—¥æœŸ (ä»…æ‹¦æˆª 1970 ä¹‹å‰æˆ–æåº¦ä¹…è¿œæœªæ¥)
        # æ³¨æ„: 2026 å¹´æ˜¯å½“å‰å¹´ä»½ï¼Œå¿…é¡»æ”¾è¡Œ!
        placeholder_dates = ["1970-01-01"]
        if any(p in s for p in placeholder_dates):
            return None
            
        from datetime import datetime as dt
        from datetime import datetime
        current_year = datetime.now().year
        
        try:
            # ä¼˜å…ˆåˆ¤æ–­ YYYY-MM-DD
            if '-' in s and len(s) >= 10:
                obj = dt.strptime(s[:10], "%Y-%m-%d")
                if obj.year > 1900 and obj.year <= current_year + 1: return obj 
            # åˆ¤æ–­çº¯æ•°å­—æ—¶é—´æˆ³
            if s.isdigit() and len(s) >= 10:
                ts = int(s)
                if len(s) == 13: ts /= 1000
                if ts > 0:
                    obj = dt.fromtimestamp(ts)
                    if obj.year > 1970 and obj.year <= current_year + 1: return obj
            # åˆ¤æ–­ YYYY
            if len(s) == 4 and s.isdigit():
                obj = dt.strptime(s, "%Y")
                if obj.year <= current_year + 1: return obj
        except: pass
        return None

    async def _embed_publish_time_to_file(self, file_path: str, publish_time: str) -> bool:
        """
        å°†å‘å¸ƒæ—¶é—´å†…åµŒåˆ°éŸ³é¢‘æ–‡ä»¶(FLAC/MP3)
        
        Args:
            file_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            publish_time: å‘å¸ƒæ—¶é—´,æ ¼å¼: YYYY-MM-DD æˆ– YYYY
        """
        logger.info(f"  ğŸ“… å†…åµŒå‘å¸ƒæ—¶é—´åˆ°æ–‡ä»¶: {file_path} -> {publish_time}")
        
        try:
            # Normalize Timestamp to YYYY-MM-DD
            # Valid Years: 1900-2100. If > 3000, likely timestamp.
            # CRITICAL: Filter out garbage dates like -28800000 or 0
            if publish_time and str(publish_time).replace('.', '').replace('-', '').isdigit():
                ts = float(publish_time)
                if ts <= 0:
                    logger.warning(f"    -> è¿‡æ»¤æ— æ•ˆæ—¥æœŸå ä½ç¬¦: {publish_time}")
                    return False
                
                if ts > 3000: # Simple check if it's a timestamp
                     if ts > 20000000000: # Likely milliseconds (2000*10^7 approx) - 13 digits
                          ts /= 1000
                     from datetime import datetime
                     publish_time = datetime.fromtimestamp(ts).strftime('%Y-%m-%d')
                     logger.info(f"    -> æ ¼å¼åŒ– timestamp ä¸ºæ—¥æœŸ: {publish_time}")
            elif not publish_time or str(publish_time).strip() == "":
                return False

            file_ext = file_path.lower().split('.')[-1]
            
            if file_ext == 'flac':
                # FLAC: ä½¿ç”¨DATEæ ‡ç­¾
                from mutagen.flac import FLAC
                audio = FLAC(file_path)
                audio['DATE'] = publish_time
                audio.save()
                logger.info(f"  âœ… FLACå‘å¸ƒæ—¶é—´å†…åµŒæˆåŠŸ")
                return True
                
            elif file_ext == 'mp3':
                # MP3: ä½¿ç”¨ID3çš„TDRC(Recording Time)æˆ–TYER(Year)å¸§
                from mutagen.id3 import ID3, TDRC, TYER
                try:
                    audio = ID3(file_path)
                except:
                    audio = ID3()
                
                # åˆ é™¤æ—§çš„æ—¥æœŸæ ‡ç­¾
                audio.delall('TDRC')
                audio.delall('TYER')
                
                # ä¼˜å…ˆä½¿ç”¨TDRC(ID3v2.4æ ‡å‡†)
                audio.add(TDRC(encoding=3, text=publish_time))
                # åŒæ—¶æ·»åŠ TYERä»¥å…¼å®¹æ—§æ’­æ”¾å™¨
                if len(publish_time) >= 4:
                    audio.add(TYER(encoding=3, text=publish_time[:4]))
                
                audio.save(file_path)
                logger.info(f"  âœ… MP3å‘å¸ƒæ—¶é—´å†…åµŒæˆåŠŸ")
                return True
            else:
                logger.warning(f"  ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}")
                return False
                
        except Exception as e:
            logger.error(f"  å‘å¸ƒæ—¶é—´å†…åµŒå¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            return False

    async def _embed_basic_metadata(self, file_path: str, title: str, artist: str, album: str) -> bool:
        """
        å†…åµŒåŸºæœ¬å…ƒæ•°æ® (Title, Artist, Album) åˆ°éŸ³é¢‘æ–‡ä»¶
        """
        import logging
        from mutagen.flac import FLAC
        from mutagen.id3 import ID3, TIT2, TPE1, TALB
        from mutagen import File as MutagenFile
        
        logger = logging.getLogger(__name__)
        
        try:
            file_ext = file_path.lower().split('.')[-1]
            
            if file_ext == 'flac':
                audio = FLAC(file_path)
                if title: audio['title'] = title
                if artist: audio['artist'] = artist
                if album: audio['album'] = album
                audio.save()
                return True
                
            elif file_ext == 'mp3':
                try:
                    audio = ID3(file_path)
                except:
                    audio = ID3()
                
                if title: audio.add(TIT2(encoding=3, text=title))
                if artist: audio.add(TPE1(encoding=3, text=artist))
                if album: audio.add(TALB(encoding=3, text=album))
                
                audio.save(file_path)
                return True
                
            return False
            
        except Exception as e:
            logger.error(f"  åŸºæœ¬å…ƒæ•°æ®å†…åµŒå¼‚å¸¸: {e}")
            return False

    async def refresh_artist(self, db: AsyncSession, artist_name: str) -> int:
        """
        Refresh songs for a specific artist from online sources.
        Fully rewritten for Normalized Schema (Song + SongSource).
        """
        from app.services.music_providers.aggregator import MusicAggregator
        from app.models.artist import Artist, ArtistSource
        from app.models.song import Song, SongSource
        from sqlalchemy import select
        from collections import defaultdict
        # logger is already defined at module level
        self._refresh_enrich_count = 0
        import logging
        local_logger = logging.getLogger(__name__)
        
        logger.info(f"Refreshing artist: {artist_name}")
        
        # 1. Get IDs for the Artist
        artist_repo = ArtistRepository(db)
        artist = await artist_repo.get_by_name(artist_name)
        if not artist:
            logger.warning(f"Artist {artist_name} not found in DB")
            return 0
            
        # ğŸŸ¢ [Optimization] Trigger local scan first to ensure local songs are in DB and linked
        logger.info(f"ğŸ“ [Pre-refresh] Scanning local files to ensure database is up-to-date for {artist_name}...")
        await self.scan_service.scan_local_files(db)
            
        # Broadcast Start
        from core.websocket import manager
        await manager.broadcast({
            "type": "artist_progress",
            "artistId": str(artist.id),
            "artistName": artist.name,
            "state": "scanning",
            "progress": 10,
            "message": "ğŸ“¥ æ­£åœ¨æ‹‰å–å…¨ç½‘æ­Œæ›²...",
            "songCount": await artist_repo.get_song_count(artist.id) # Initial count
        })
            
        # Load sources
        stmt = select(ArtistSource).where(ArtistSource.artist_id == artist.id)
        sources = (await db.execute(stmt)).scalars().all()
        
        artist_ids = {s.source: s.source_id for s in sources}
        
        if not artist_ids:
            logger.info("No source IDs found for artist, skipping fetch.")
            return 0
            
        # 2. Fetch Raw Songs (Sequential for granular reporting as requested)
        aggregator = MusicAggregator()
        raw_songs = []
        
        # 2.1 QQ Music
        if 'qqmusic' in artist_ids:
            await manager.broadcast({
                "type": "artist_progress",
                "artistId": str(artist.id),
                "artistName": artist.name,
                "state": "fetching_qq",
                "progress": 20,
                "message": "ğŸ“¥ æ­£åœ¨æ‹‰å– QQ éŸ³ä¹æ­Œæ›²åˆ—è¡¨..."
            })
            qq_songs = await aggregator.providers[1].get_artist_songs(artist_ids['qqmusic'], limit=1000)
            raw_songs.extend(qq_songs)
            logger.info(f"Fetched {len(qq_songs)} songs from QQ Music")

        # 2.2 Netease
        if 'netease' in artist_ids:
            await manager.broadcast({
                "type": "artist_progress",
                "artistId": str(artist.id),
                "artistName": artist.name,
                "state": "fetching_netease",
                "progress": 30,
                "message": "ğŸ“¥ æ­£åœ¨æ‹‰å–ç½‘æ˜“äº‘éŸ³ä¹æ­Œæ›²åˆ—è¡¨..."
            })
            netease_songs = await aggregator.providers[0].get_artist_songs(artist_ids['netease'], limit=1000)
            raw_songs.extend(netease_songs)
            logger.info(f"Fetched {len(netease_songs)} songs from Netease")
        
        # 2.3 Filter dirty songs
        raw_songs = [s for s in raw_songs if aggregator._is_valid_song(s)]
        
        if not raw_songs:
            return 0
            
        # [Fallback] Update Avatar if missing (using song metadata as source)
        if not artist.avatar:
            for rs in raw_songs:
                if rs.cover:
                    artist.avatar = rs.cover
                    logger.info(f"ğŸ¨ å·²ä»é‡‡é›†åˆ—è¡¨è‡ªåŠ¨è¡¥å…¨è‰ºäººå¤´åƒ: {artist_name}")
                    # Note: We commit later or now? Consolidating to now for visual feedback.
                    await db.commit()
                    break
            
        # 3. Progress Refresh
        await manager.broadcast({
            "type": "artist_progress",
            "artistId": str(artist.id),
            "artistName": artist.name,
            "state": "matching",
            "progress": 40,
            "message": f"è·å–åˆ° {len(raw_songs)} é¦–æ­Œæ›²ï¼Œæ­£åœ¨èšåˆ..."
        })


        # 3. [New] Reverse Lookup for Missing Originals (Instrumental -> Original)
        # ç”¨æˆ·ç—›ç‚¹: ç½‘æ˜“äº‘çƒ­æ­Œæ¦œä¸å…¨ï¼Œåªæœ‰ä¼´å¥(Inst)æ²¡æœ‰åŸç‰ˆã€‚
        # è§£å†³æ–¹æ¡ˆ: æ‰«ææ‰€æœ‰ä¼´å¥ï¼Œå¦‚æœæ‰¾ä¸åˆ°åŸç‰ˆï¼Œå¼ºåˆ¶åˆ©ç”¨ Search API åæŸ¥åŸç‰ˆã€‚
        
        # 3.1 Build quick lookup set
        existing_titles_norm = {
            self._normalize_cn_brackets(s.title).lower().strip() for s in raw_songs
        }
        
        extra_songs = []
        checked_inst_titles = set()
        
        inst_keywords = ['(ä¼´å¥)', '(inst)', 'instrumental', 'ä¼´å¥', 'inst.']
        
        for s in raw_songs:
            title_lower = s.title.lower()
            is_inst = False
            clean_title = s.title
            
            # Detect Inst
            for kw in inst_keywords:
                if kw in title_lower:
                    is_inst = True
                    # Clean title: Replace kw, remove brackets if empty
                    # Simple replace logic
                    clean_title = s.title.lower().replace(kw, '').strip()
                    clean_title = clean_title.replace('()', '').replace('ï¼ˆï¼‰', '').strip()
                    
                    # Try to restore case? No need for search query usually.
                    # But s.title might use arbitrary case. Let's use clean lower for check.
                    # Better use the norm function
                    break
            
            if is_inst:
                norm_clean = self._normalize_cn_brackets(clean_title).lower().strip()
                
                # Check duplication and if already checked
                if norm_clean not in existing_titles_norm and norm_clean not in checked_inst_titles:
                    checked_inst_titles.add(norm_clean)
                    
                    # TRIGGER REVERSE LOOKUP
                    logger.info(f"ğŸ” å‘ç°å­¤ç«‹ä¼´å¥ '{s.title}', å°è¯•åæŸ¥åŸç‰ˆ: '{clean_title}'")
                    
                    await manager.broadcast({
                        "type": "artist_progress",
                        "artistId": str(artist.id),
                        "artistName": artist.name,
                        "state": "matching",
                        "progress": 40,
                        "message": f"æ­£åœ¨è¡¥å…¨åŸç‰ˆ: {clean_title}..."
                    })
                    
                    # Use aggregator search (try both providers? or just one?)
                    # Aggregator doesn't have convenient search_song that aggregates?
                    # Providers act independently. Let's try Netease first as it's the likely source.
                    # Or reuse metadata_service logic? MetadataService is wrapper.
                    # Let's use aggregator.providers[0] (Netease) directly.
                    
                    try:
                        # Use provider 0 (Netease)
                        query = f"{clean_title} {artist_name}"
                        # Limit 3 ensures we get best match
                        search_res = await aggregator.providers[0].search_song(query, limit=3)
                        
                        found_target = None
                        for cand in search_res:
                            # Strict title match to avoid getting another remix
                            cand_norm = self._normalize_cn_brackets(cand.title).lower().strip()
                            if cand_norm == norm_clean:
                                found_target = cand
                                break
                        
                        if found_target:
                            logger.info(f"  âœ… æˆåŠŸæ‰¾å›åŸç‰ˆ: {found_target.title} (ID: {found_target.id})")
                            extra_songs.append(found_target)
                            existing_titles_norm.add(norm_clean) # Prevent researching
                    except Exception as e:
                        logger.warning(f"  âŒ åæŸ¥å¤±è´¥: {e}")

        if extra_songs:
            raw_songs.extend(extra_songs)
            logger.info(f"âœ¨ åå‘è¡¥å…¨äº† {len(extra_songs)} é¦–ç¼ºå¤±çš„åŸç‰ˆæ­Œæ›²")

        # 4. Matching Local
        await manager.broadcast({
            "type": "artist_progress",
            "artistId": str(artist.id),
            "artistName": artist.name,
            "state": "matching",
            "progress": 50,
            "message": "ğŸ” æ­£åœ¨ä¸æœ¬åœ°åº“åŒ¹é…..."
        })
        
        # Group by Title + Album (Smart Merge)
        grouped_songs = defaultdict(list)
        for s in raw_songs:
            # Generate Key: Clean Title + Local Normalize
            clean_title = self._normalize_cn_brackets(s.title).lower().strip()
            grouped_songs[clean_title].append(s)
            
        new_count = 0
        total_groups = len(grouped_songs)
        processed = 0
        
        # 5. Process Groups -> Logical Song + SongSources
        # Sort groups by publish time (newest first) to ensure logical insertion order
        def get_group_date(group):
            dates = [str(getattr(s, 'publish_time', '0000-00-00')) for s in group if getattr(s, 'publish_time', None)]
            return max(dates) if dates else '0000-00-00'
            
        sorted_groups = sorted(grouped_songs.items(), key=lambda x: get_group_date(x[1]), reverse=True)
        
        song_repo = SongRepository(db)
        all_db_songs = await song_repo.get_by_artist(artist.id)
        db_song_map = {
            self._normalize_cn_brackets(s.title).lower().strip(): s 
            for s in all_db_songs
        }
        logger.info(f"  ğŸ” å·²ç¼“å­˜ {len(all_db_songs)} é¦–ç°æœ‰æ­Œæ›²ç”¨äºæ¨¡ç³ŠåŒ¹é…")

        for title_key, group in sorted_groups:
            processed += 1
            # Determine Best Meta (Priority: QQ > Netease)
            best_meta = group[0]
            qq_ver = next((x for x in group if x.source == 'qqmusic'), None)
            if qq_ver: best_meta = qq_ver
                
            # 5.1 å™ªå£°è¿‡æ»¤ (ä¼˜åŒ–ï¼šæ”¾å®½é™åˆ¶é˜²æ­¢è¯¯æ€å•æ›²)
            # åªæœ‰å½“æ ‡é¢˜åŒ…å« '#' (é€šå¸¸æ˜¯åŠ¨æ€) ä¸”æ²¡æœ‰ä¸“è¾‘æ—¶æ‰è¿‡æ»¤ã€‚
            # "ä»Šå¤©"ã€"æ˜å¤©"ã€"å®£ä¼ " ç­‰è¯åœ¨æ­Œåä¸­å˜å¸¸è§äº†ï¼Œä¸å†ç›´æ¥è¿‡æ»¤ï¼Œé™¤éå®ƒæ˜¯çº¯åŠ¨æ€ã€‚
            noise_keywords = ["#", "å·¡æ¼”", "æœ€åä¸€ç«™", "é¢„å‘Š"]
            if any(k in best_meta.title for k in noise_keywords) and not best_meta.album and len(best_meta.title) > 30:
                logger.info(f"ğŸ§¹ è¿‡æ»¤å™ªå£°åŠ¨æ€: {best_meta.title}")
                continue

            # Find/Create Logical Song by Fuzzy Match
            norm_key = self._normalize_cn_brackets(best_meta.title).lower().strip()
            existing_song = db_song_map.get(norm_key)
            
            if not existing_song:
                existing_song = Song(
                    artist_id=artist.id,
                    title=best_meta.title,
                    album=best_meta.album,
                    created_at=datetime.now(),
                    status="PENDING"
                )
                db.add(existing_song)
                await db.flush() # ID
                new_count += 1
                db_song_map[norm_key] = existing_song
            
            # Update Meta (Smart Merge)
            candidate_covers = []
            candidate_dates = []
            candidate_albums = []
            
            for s in group:
                c_url = getattr(s, 'cover_url', None) or getattr(s, 'pic_url', None)
                if c_url: candidate_covers.append(c_url)
                # ä»…ä¿ç•™æœ‰æ•ˆçš„å€™é€‰æ—¥æœŸ
                p_raw = getattr(s, 'publish_time', None)
                if p_raw:
                    p_parsed = self._parse_date(str(p_raw))
                    if p_parsed: candidate_dates.append(p_parsed.strftime("%Y-%m-%d"))
                
                alb = getattr(s, 'album', None)
                if alb: candidate_albums.append(alb)
            
            # --- æ—§æ›²æ²»æ„ˆä¸æ™ºèƒ½è¡¥å…¨ ---
            # åˆ¤æ–­å½“å‰æ­Œæ›²æ˜¯å¦éœ€è¦â€œè¡¥å…¨â€æˆ–â€œä¿®æ­£â€
            needs_healing = False
            if not existing_song.cover or 'gtimg.cn' in str(existing_song.cover):
                needs_healing = True
            if not existing_song.publish_time or existing_song.publish_time.year >= 2026 or existing_song.publish_time.year <= 1970:
                needs_healing = True
            if not existing_song.album:
                needs_healing = True
                
            # å¦‚æœç°æœ‰æ•°æ®ä¸å…¨ï¼Œä¸”å€™é€‰é›†ä¸­ä¹Ÿæ²¡æŠ“åˆ°æœ‰æ•ˆä¿¡æ¯ï¼Œè§¦å‘å…¨ç½‘è¡¥å…¨
            if needs_healing and (not candidate_covers or not candidate_dates):
                if not hasattr(self, '_refresh_enrich_count'): self._refresh_enrich_count = 0
                if self._refresh_enrich_count < 15: # é€‚åº¦æ”¾å®½è¡¥å…¨é™åˆ¶
                    logger.info(f"  ğŸ” æ­£åœ¨ä¸º [æ—§æ›²] å°è¯•å…¨ç½‘è¡¥å…¨å…ƒæ•°æ®: {best_meta.title}")
                    try:
                        enriched = await aggregator.get_song_metadata_from_best_source(best_meta.title, artist.name)
                        if enriched:
                            if enriched.get('cover_url'): candidate_covers.insert(0, enriched['cover_url'])
                            if enriched.get('publish_time'):
                                p_enrich = self._parse_date(str(enriched['publish_time']))
                                if p_enrich: candidate_dates.insert(0, p_enrich.strftime("%Y-%m-%d"))
                            if enriched.get('album') and not candidate_albums: candidate_albums.append(enriched['album'])
                            self._refresh_enrich_count += 1
                    except Exception as e:
                        logger.warning(f"Metadata healing failed for {best_meta.title}: {e}")

            # --- åº”ç”¨æ›´æ–° ---
            # 1. æ›´æ–°å°é¢
            if candidate_covers:
                # å¦‚æœç°æœ‰å°é¢ä¸ºç©ºï¼Œæˆ–è€…ç›®å‰çš„åªæ˜¯ QQ éŸ³ä¹çš„ä½è´¨å›¾/å¤±æ•ˆå›¾ï¼Œåˆ™æ›´æ–°
                if not existing_song.cover or 'gtimg.cn' in str(existing_song.cover):
                    existing_song.cover = candidate_covers[0]
            
            # 2. æ›´æ–°ä¸“è¾‘
            if candidate_albums and not existing_song.album:
                existing_song.album = candidate_albums[0]

            # 3. æ›´æ–°/ä¿®æ­£æ—¥æœŸ
            new_date = None
            if candidate_dates:
                # ä¼˜å…ˆé‡‡ç”¨è§£æåçš„ç¬¬ä¸€ä¸ªæœ‰æ•ˆæ—¥æœŸ
                new_date = self._parse_date(candidate_dates[0])
            
            # ä¼´å¥ç‰ˆæœ¬å›é€€ç­–ç•¥: å¦‚æœä¼´å¥ç‰ˆæ²¡æŠ“åˆ°æ—¥æœŸ, å°è¯•ç»§æ‰¿åŸç‰ˆçš„æ—¥æœŸ
            if not new_date and "_inst" in norm_key:
                orig_key = norm_key.replace("_inst", "")
                if orig_key in db_song_map:
                    orig_song = db_song_map[orig_key]
                    if orig_song.publish_time and 1970 < orig_song.publish_time.year < 2026:
                        new_date = orig_song.publish_time
                        logger.info(f"    ğŸ¹ ä¼´å¥æ—¥æœŸå›é€€: {best_meta.title} -> ç»§æ‰¿åŸç‰ˆ ({new_date.strftime('%Y-%m-%d')})")

            if new_date:
                # åªæœ‰åœ¨ç°æœ‰æ—¥æœŸæ— æ•ˆï¼ˆå ä½ç¬¦ï¼‰æˆ–æ˜¾è‘—å·®å¼‚æ—¶æ‰è¦†ç›–
                curr_date = existing_song.publish_time
                if not curr_date or curr_date.year >= 2026 or curr_date.year <= 1970:
                    existing_song.publish_time = new_date
                    logger.info(f"    ğŸ“… æ—¥æœŸä¿®æ­£: {best_meta.title} -> {new_date.strftime('%Y-%m-%d')}")
                elif abs((curr_date - new_date).days) > 1:
                    # å¦‚æœå·®å¼‚å¾ˆå¤§ï¼Œå¯èƒ½ä¹‹å‰çš„æŠ“å–æœ‰è¯¯ï¼Œä»¥æœ¬æ¬¡æŠ“å–ä¸ºå‡†
                    existing_song.publish_time = new_date

            # Update Sources
            if "æˆ‘ä¸è¦åŸè°…ä½ " in best_meta.title:
                # å·²ç§»é™¤ debug log
                pass
            
            # Update Sources
            for s in group:
                chk = select(SongSource).where(
                    SongSource.song_id == existing_song.id, 
                    SongSource.source == s.source,
                    SongSource.source_id == str(s.id)
                )
                if not (await db.execute(chk)).scalars().first():
                    src_ent = SongSource(
                        song_id=existing_song.id,
                        source=s.source,
                        source_id=str(s.id),
                        cover=s.cover_url or getattr(s, 'pic_url', None),
                        duration=s.duration,
                        url=getattr(s, 'url', None),
                        data_json = {'quality': getattr(s, 'quality', 'unknown')}
                    )
                    db.add(src_ent)
            
            # Progress Broadcast (Matching)
            if processed % 20 == 0 or processed == total_groups:
                await manager.broadcast({
                    "type": "artist_progress",
                    "artistId": str(artist.id),
                    "artistName": artist.name,
                    "state": "matching",
                    "progress": int(40 + (processed / total_groups) * 35),
                    "message": f"â³ åŒ¹é…è¿›åº¦ ({processed}/{total_groups})"
                })
                    
        await db.commit()
        
        # 6. Orphan Rescue: æŒ½æ•‘æœ¬åœ°å­¤å„¿æ­Œæ›² (é’ˆå¯¹ä¸åœ¨çƒ­é—¨åˆ—è¡¨ä¸­çš„ä¼´å¥/å†·é—¨æ­Œæ›²)
        # æŸ¥æ‰¾è¯¥æ­Œæ‰‹ä¸‹åªæœ‰æœ¬åœ°æºçš„æ­Œæ›²
        try:
            from sqlalchemy.orm import selectinload
            stmt = select(Song).options(selectinload(Song.sources)).where(
                Song.artist_id == artist.id,
                Song.local_path != None
            )
            local_songs = (await db.execute(stmt)).scalars().all()
            
            logger.info(f"ğŸš‘ [æŒ½æ•‘æ¨¡å¼] å¼€å§‹æ£€æŸ¥ {len(local_songs)} é¦–æœ¬åœ°æ­Œæ›²... (Artist: {artist.name})")
            
            # Notify User: Starting Rescue
            from core.websocket import manager
            await manager.broadcast({
                 "type": "artist_progress",
                 "artistId": str(artist.id),
                 "artistName": artist.name,
                 "state": "rescue",
                 "progress": 80,
                 "message": f"æ­£åœ¨æ£€æŸ¥ {len(local_songs)} é¦–æœ¬åœ°å­¤å„¿æ­Œæ›²..."
            })
            
            rescue_count = 0
            
            # Define helper for matching
            def _find_match(candidates, local_song):
                norm_local = self._normalize_cn_brackets(local_song.title).lower().strip()
                
                # Check for Variant Keywords in Local Title (Safeguard for "Accompaniment" bug)
                variant_keywords = ["(ä¼´å¥)", " ä¼´å¥", "inst.", "instrumental", "demo", "(live)", " live", "ï¼ˆä¼´å¥ï¼‰"]
                is_local_variant = any(k in local_song.title.lower() for k in variant_keywords)
                
                for res in candidates:
                    norm_res = self._normalize_cn_brackets(res.title).lower().strip()
                    
                    # Check for Variant Keywords in Remote Title
                    is_remote_variant = any(k in res.title.lower() for k in variant_keywords)
                    
                    logger.info(f"    ğŸ” æ¯”å¯¹: Local='{norm_local}' vs Remote='{norm_res}' ('{res.title}')")
                    
                    # 1. Strict Match
                    if norm_local == norm_res:
                        logger.info(f"      -> âœ… ç²¾ç¡®åŒ¹é…æˆåŠŸ: '{res.title}'")
                        return res
                    
                    # 2. Fuzzy/Substring Match (Remote in Local)
                    if norm_res in norm_local:
                        # CRITICAL SAFEGUARD: 
                        # If Local says "Accompaniment" but Remote is "Original", DO NOT MATCH!
                        if is_local_variant and not is_remote_variant:
                            logger.info(f"      â›” æ‹’ç»æ¨¡ç³ŠåŒ¹é…: æœ¬åœ°æ˜¯å˜ä½“('{local_song.title}') ä½†è¿œç¨‹æ˜¯åŸç‰ˆ('{res.title}')")
                            continue
                        
                        logger.info(f"      -> âš ï¸ æ¨¡ç³ŠåŒ¹é…æˆåŠŸ(Remoteåœ¨Localä¸­): '{res.title}'")
                        return res
                        
                    # 3. Reverse Fuzzy (Local in Remote)
                    # e.g. Local="Title", Remote="Title (Live)" -> Match
                    if len(norm_local) > 1 and norm_local in norm_res:
                        # Safeguard
                        if not is_local_variant and is_remote_variant:
                              logger.info(f"      âš ï¸ å…è®¸åå‘æ¨¡ç³ŠåŒ¹é…(Localåœ¨Remoteä¸­): '{res.title}' (å¯èƒ½æ˜¯Liveç‰ˆ)")
                              return res
                              
                        logger.info(f"      -> âš ï¸ æ¨¡ç³ŠåŒ¹é…æˆåŠŸ(Localåœ¨Remoteä¸­): '{res.title}'")
                        return res
                return None

            for song in local_songs:
                # æ£€æŸ¥æ˜¯å¦å·²æœ‰å…³è”åœ¨çº¿æº (QQ/Netease)
                has_online = any(s.source in ['qqmusic', 'netease'] for s in song.sources)
                
                # å¦‚æœå·²æœ‰æºä¸”æœ‰å‘å¸ƒæ—¥æœŸï¼Œè§†ä¸ºå®Œæ•´ï¼Œè·³è¿‡. 
                # (å…è®¸æŒ½æ•‘ has_online=True ä½† publish_time=None çš„æƒ…å†µ)
                if has_online and song.publish_time:
                    continue
                
                logger.info(f"  ğŸ‘‰ æ­£åœ¨å°è¯•æŒ½æ•‘: {song.title}")
                
                 # 0. Try to match against In-Memory Processed Songs (Fastest)
                match_title = self._normalize_cn_brackets(song.title).lower().strip()
                
                # Refine Title from Local Tags if possible (User says "File has info")
                # Sometimes DB title is from Filename which is bad, but Tags are good.
                if song.local_path:
                    try:
                        from tinytag import TinyTag
                        import os
                        if os.path.exists(song.local_path):
                            tag = TinyTag.get(song.local_path)
                            if tag and tag.title:
                                t_clean = self._normalize_cn_brackets(tag.title).lower().strip()
                                if t_clean and t_clean != match_title:
                                    logger.info(f"    ğŸ·ï¸ ä½¿ç”¨å†…åµŒæ ‡ç­¾æ ‡é¢˜è¿›è¡ŒåŒ¹é…: {tag.title}")
                                    # Create a temporary fake song object to pass to _find_match logic if needed, 
                                    # or just update song.title temporarily? 
                                    # Better: Update the 'local_song' object passed to _find_match? NO, don't mutate DB obj yet.
                                    # We can just trust _find_match logic which reads song.title.
                                    # Let's temporarily update song.title for the match function?
                                    # Or better, pass explicit title to _find_match? _find_match uses song.title.
                                    # Let's override song.title (in memory only, unless we commit)
                                    song.title = tag.title 
                    except: pass

                # Optimization: Scan raw_songs in memory for a fuzzy match
                best_match = _find_match(raw_songs, song)
                
                if best_match:
                    logger.info(f"    ğŸš€ å†…å­˜å¿«é€Ÿå‘½ä¸­ (ç¼“å­˜): {best_match.title}")
                else:
                    # 1. Active Search (Strict Key) - Only if not found in memory
                    search_key = f"{song.title} {artist.name}"
                    search_results = await self.aggregator.search_song(search_key, limit=5)
                    best_match = _find_match(search_results, song)
                
                # 2. Strategy 2: Relaxed Search (Remove brackets) if strict match failed
                if not best_match:
                    import re
                    # Remove content in brackets: (xxx) or [xxx] or ã€xxxã€‘ or ï¼ˆxxxï¼‰
                    clean_title = re.sub(r"[\(\[ã€ï¼ˆ].*?[\)\]ã€‘ï¼‰]", "", song.title).strip()
                    # Only try if title actually changed (had brackets)
                    if clean_title and clean_title != song.title:
                        logger.info(f"    âš ï¸ æœªå‘½ä¸­ï¼Œå°è¯•å»æ‹¬å·æœç´¢: '{clean_title}'")
                        clean_key = f"{clean_title} {artist.name}"
                        relaxed_results = await self.aggregator.search_song(clean_key, limit=5)
                        best_match = _find_match(relaxed_results, song)
                            
                if best_match:
                     import json
                     
                     # Check if source exists to avoid duplicate constraint error
                     chk = select(SongSource).where(
                         SongSource.song_id == song.id,
                         SongSource.source == best_match.source,
                         SongSource.source_id == str(best_match.id)
                     )
                     existing_src = (await db.execute(chk)).scalars().first()
                     
                     if not existing_src:
                         # Add Source
                         new_source = SongSource(
                             song_id=song.id,
                             source=best_match.source,
                             source_id=best_match.id,
                             cover=best_match.cover_url,
                             duration=best_match.duration,
                             url="",
                             data_json=json.dumps(best_match.__dict__, default=str)
                         )
                         db.add(new_source)
                         logger.info(f"    ğŸ”— å…³è”æˆåŠŸ! æº: {best_match.source} - {best_match.title}")
                     else:
                         logger.info(f"    ğŸ”— å·²å­˜åœ¨æº {best_match.source}ï¼Œä»…æ£€æŸ¥å…ƒæ•°æ®æ›´æ–°")
                     
                     # é¡ºä¾¿è¡¥å…¨å…ƒæ•°æ® (å¼ºåˆ¶æ£€æŸ¥å³ä¾¿å·²æœ‰å¯¹åº”æºï¼Œä»¥åŒæ­¥å°é¢å’Œæ—¥æœŸ)
                     if not song.cover and best_match.cover_url:
                         song.cover = best_match.cover_url
                         logger.info(f"    ğŸ–¼ï¸ è¡¥å…¨å°é¢: {song.cover[:50]}...")
                         
                     if not song.album and best_match.album:
                         song.album = best_match.album
                         logger.info(f"    ğŸ’½ è¡¥å…¨ä¸“è¾‘: {song.album}")
                         
                     # è¡¥å…¨å‘å¸ƒæ—¶é—´
                     if not song.publish_time and best_match.publish_time:
                         try:
                              pt_str = str(best_match.publish_time).strip()
                              from datetime import datetime as dt
                              # Handle standard formats
                              if pt_str.replace('-', '').isdigit() and len(pt_str) >= 10:
                                   ts = int(pt_str)
                                   if len(pt_str) == 13: ts = ts / 1000
                                   if ts > 0:
                                       song.publish_time = dt.fromtimestamp(ts)
                              elif len(pt_str) == 4 and pt_str.isdigit(): 
                                  song.publish_time = dt.strptime(pt_str, "%Y")
                              elif len(pt_str) >= 10: 
                                  dt_obj = dt.strptime(pt_str[:10], "%Y-%m-%d")
                                  if dt_obj.year > 1970:
                                      song.publish_time = dt_obj
                              if song.publish_time:
                                  logger.info(f"    ğŸ“… è¡¥å…¨æ—¥æœŸ: {song.publish_time}")
                         except Exception as e:
                             logger.warning(f"Date parse failed for rescued song {song.title}: {e}")

                     rescue_count += 1
            
            if rescue_count > 0:
                await db.commit()
                logger.info(f"âœ¨ æŒ½æ•‘è¡ŒåŠ¨ç»“æŸ: æˆåŠŸä¿®å¤ {rescue_count} é¦–æ­Œæ›²")

            # ==========================================
            # 7. å…¨åº“æ²»æ„ˆ (Global Healing)
            # ==========================================
            # æ‰«ææ•°æ®åº“ä¸­è¯¥æ­Œæ‰‹çš„æ‰€æœ‰æ­Œæ›²ï¼Œä¿®å¤æ®‹ç•™çš„ 2026-01-07 å ä½ç¬¦å’Œç¼ºå¤±å°é¢
            # ==========================================
            logger.info("ğŸ¥ å¯åŠ¨å…¨åº“å…ƒæ•°æ®æ²»æ„ˆ...")
            heal_count = 0
            
            # è·å–æ‰€æœ‰æ­Œæ›² (è½¬ä¸º List ä»¥å…æ¸¸æ ‡é—®é¢˜)
            res = await db.execute(select(Song).where(Song.artist_id == artist.id))
            all_db_songs = res.scalars().all()
            
            # å»ºç«‹ Title -> Song æ˜ å°„ï¼Œç”¨äºä¼´å¥å›é€€æŸ¥æ‰¾åŸç‰ˆ
            title_map = {s.title: s for s in all_db_songs}
            
            for s in all_db_songs:
                needs_update = False
                
                # 1. æ£€æŸ¥æ—¥æœŸæ˜¯å¦æœ‰æ•ˆ
                is_invalid_date = False
                if not s.publish_time:
                    is_invalid_date = True
                else:
                    # æ£€æŸ¥å ä½ç¬¦å¹´ä»½ (å…è®¸å½“å¹´çš„)
                    y = s.publish_time.year
                    if y > datetime.now().year + 1 or y <= 1970:
                        is_invalid_date = True
                    # æ£€æŸ¥ '01-01' ç­‰å¸¸è§é»˜è®¤å€¼ (å¦‚æœæ˜¯1970)
                
                # 2. æ£€æŸ¥å°é¢
                is_missing_cover = not s.cover or 'gtimg.cn' in str(s.cover) or 'placeholder' in str(s.cover)
                
                # --- A. ä¼´å¥ç‰ˆå›é€€ç­–ç•¥ (ä¼˜å…ˆå°è¯•æœ¬åœ°ç»§æ‰¿) ---
                if is_invalid_date and ("ä¼´å¥" in s.title or "Inst" in s.title):
                    # å°è¯•æŸ¥æ‰¾åŸç‰ˆ
                    # ç®€å•å»é™¤å…³é”®å­—
                    import re
                    # ç§»é™¤ (ä¼´å¥) [ä¼´å¥] ç­‰
                    orig_title = re.sub(r"[\(\[ã€ï¼ˆ].*?(ä¼´å¥|Inst|Backing).*?[\)\]ã€‘ï¼‰]", "", s.title, flags=re.IGNORECASE).strip()
                    if orig_title in title_map:
                        orig = title_map[orig_title]
                        if orig.publish_time and 1970 < orig.publish_time.year <= datetime.now().year + 1:
                            s.publish_time = orig.publish_time
                            needs_update = True
                            is_invalid_date = False # å·²ä¿®å¤
                            logger.info(f"    ğŸ¹ [æ²»æ„ˆ] ä¼´å¥ç»§æ‰¿åŸç‰ˆæ—¥æœŸ: {s.title} -> {s.publish_time}")

                # --- B. å…¨ç½‘è¡¥å…¨ (å¦‚æœæ˜¯å­¤å„¿æ­Œæ›²æˆ–ä»æœªä¿®å¤) ---
                # ä»…å½“: æ—¥æœŸä»æ— æ•ˆ OR å°é¢ç¼ºå¤±
                if is_invalid_date or is_missing_cover:
                    # é¢‘ç‡é™åˆ¶: å¤ç”¨ _refresh_enrich_countï¼Œç¨å¾®æ”¾å®½ä¸€ç‚¹ä¸Šé™ç»™æ²»æ„ˆæµç¨‹
                    if not hasattr(self, '_refresh_enrich_count'): self._refresh_enrich_count = 0
                    if self._refresh_enrich_count < 20: 
                        logger.info(f"    ğŸ©º [æ²»æ„ˆ] æ­£åœ¨å…¨ç½‘æ£€ç´¢: {s.title} (Reason: Date={not is_invalid_date}, Cover={not is_missing_cover})")
                        try:
                            # æœç´¢
                            meta = await aggregator.get_song_metadata_from_best_source(s.title, artist.name)
                            if meta:
                                # ä¿®å¤æ—¥æœŸ
                                if is_invalid_date and meta.get('publish_time'):
                                    p_parsed = self._parse_date(str(meta['publish_time']))
                                    if p_parsed:
                                        s.publish_time = p_parsed
                                        needs_update = True
                                        logger.info(f"    ğŸ“… [æ²»æ„ˆ] æ—¥æœŸä¿®å¤: {p_parsed}")
                                
                                # ä¿®å¤å°é¢
                                if is_missing_cover and meta.get('cover_url'):
                                    s.cover = meta['cover_url']
                                    needs_update = True
                                    logger.info(f"    ğŸ–¼ï¸ [æ²»æ„ˆ] å°é¢ä¿®å¤: {s.cover[:30]}...")
                                    
                                # ä¿®å¤ä¸“è¾‘
                                if not s.album and meta.get('album'):
                                    s.album = meta['album']
                                    needs_update = True
                                
                                self._refresh_enrich_count += 1
                        except Exception as e:
                            logger.warning(f"Heal failed for {s.title}: {e}")
                
                if needs_update:
                    heal_count += 1
            
            if heal_count > 0:
                await db.commit()
                logger.info(f"âœ¨ å…¨åº“æ²»æ„ˆå®Œæˆ: ä¿®å¤äº† {heal_count} é¦–æ­Œæ›²çš„å…ƒæ•°æ®")
            else:
                logger.info("âœ¨ å…¨åº“æ²»æ„ˆå®Œæˆ: æ‰€æœ‰æ­Œæ›²çŠ¶æ€è‰¯å¥½")
                
        except Exception as e:
             logger.error(f"âŒ æŒ½æ•‘æ¨¡å¼å‘ç”Ÿæ„å¤–é”™è¯¯: {e}", exc_info=True)

        # 7. Sorting & Statistics
        await manager.broadcast({
            "type": "artist_progress",
            "artistId": str(artist.id),
            "artistName": artist.name,
            "state": "statistics",
            "progress": 95,
            "message": "ğŸ“Š æ­£åœ¨è¿›è¡Œæœ€ç»ˆæ’åºä¸ç»Ÿè®¡..."
        })
        
        total_count = await artist_repo.get_song_count(artist.id)
        logger.info(f"Artist {artist_name} refresh complete. Added {new_count} new songs. Total {total_count}.")
        
        # 8. Complete & Refresh Notification
        await manager.broadcast({
             "type": "artist_progress",
             "artistId": str(artist.id),
             "artistName": artist.name,
             "state": "complete",
             "progress": 100,
             "message": f"âœ… åˆ·æ–°å®Œæˆ (æ–°å¢ {new_count} é¦–, æ€»è®¡ {total_count} é¦–)",
             "songCount": total_count  # Ensure frontend gets the latest number!
        })
        
        # Extra broadcast to trigger song list refresh on frontend
        # Trying common command names to ensure compatibility
        await manager.broadcast({
            "type": "refresh_list",
            "artistId": str(artist.id),
            "artistName": artist.name
        })
        await manager.broadcast({
            "type": "refresh_songs",
            "artistId": str(artist.id),
            "artistName": artist.name
        })
        
        return new_count



    async def delete_artist(self, db: AsyncSession, artist_id: int = None, artist_name: str = None) -> bool:
        """åˆ é™¤æ­Œæ‰‹åŠå…¶èµ„æº"""
        from app.services.subscription import SubscriptionService
        from app.models.artist import Artist
        from sqlalchemy import select
        
        # If artist_id provided
        if artist_id:
            return await SubscriptionService.delete_artist(db, artist_id)
            
        # If name provided (legacy support)
        if artist_name:
             # Find ID
             stmt = select(Artist).where(Artist.name == artist_name)
             artist = (await db.execute(stmt)).scalars().first()
             if artist:
                 return await SubscriptionService.delete_artist(db, artist.id)
        return False

    async def apply_metadata_match(self, db: AsyncSession, song_id: int, target_source: str, target_song_id: str):
        """
        æ‰‹åŠ¨åº”ç”¨å…ƒæ•°æ®åŒ¹é… (Manual Match)
        """
        from app.models.song import Song
        import datetime
        import anyio
    async def apply_metadata_match(
        self, 
        db: AsyncSession, 
        song_id: int, 
        target_source: str, 
        target_song_id: str
    ) -> bool:
        """
        åº”ç”¨æ‰‹åŠ¨åŒ¹é…çš„å…ƒæ•°æ®
        
        æµç¨‹:
        1. è·å–ç›®æ ‡æ­Œæ›²å…ƒæ•°æ®
        2. ä¸‹è½½å°é¢
        3. æ›´æ–°æ•°æ®åº“
        4. å†™å…¥æ–‡ä»¶æ ‡ç­¾ (ID3/FLAC)
        5. å†…åµŒæ­Œè¯
        """
        try:
            # 1. Get Song
            song_repo = SongRepository(db)
            song = await song_repo.get(song_id)
            if not song:
                logger.error(f"Song {song_id} not found")
                return False

            # 2. Delegate to ScraperService
            # We instantiate services here (could be injected or singleton in larger app)
            metadata_service = MetadataService()
            scraper = ScraperService(self.aggregator, metadata_service)
            
            success = await scraper.scrape_and_apply(db, song, target_source, target_song_id)
            
            if success:
                logger.info(f"Manual match applied successfully for song {song_id}")
                return True
            else:
                logger.error(f"Scraper failed for song {song_id}")
                return False

        except Exception as e:
            logger.error(f"Apply metadata match failed: {e}")
            return False

    async def reset_database(self, db: AsyncSession) -> bool:
        """é‡ç½®æ•°æ®åº“ (Clear all Songs/Artists)"""
        import logging
        from sqlalchemy import delete
        logger = logging.getLogger(__name__)
        
        try:
            from app.models.song import Song, SongSource
            from app.models.artist import Artist, ArtistSource
            
            # Delete all
            await db.execute(delete(SongSource))
            await db.execute(delete(Song))
            await db.execute(delete(ArtistSource))
            await db.execute(delete(Artist))
            
            await db.commit()
            return True
        except Exception as e:
            logger.error(f"Reset DB failed: {e}")
            await db.rollback()
            return False

    async def redownload_song(
        self, 
        db: AsyncSession, 
        song_id: int, 
        source: str, 
        source_id: str, 
        quality: int = 999,
        title: str = None,
        artist: str = None
    ) -> bool:
        """
        é‡æ–°ä¸‹è½½æ­Œæ›² (Re-download)
        
        æµç¨‹:
        1. è·å–ä¸‹è½½é“¾æ¥å¹¶ä¸‹è½½æ–°éŸ³é¢‘æ–‡ä»¶
        2. æ›´æ–°æ•°æ®åº“ local_path
        3. åˆ é™¤æ—§æ–‡ä»¶ (å¦‚æœè·¯å¾„ä¸åŒ)
        4. è°ƒç”¨ ScraperService é‡æ–°åˆ®å‰Šå…ƒæ•°æ® (ä¿è¯å°é¢/æ­Œè¯/Tagä¸æ–°éŸ³é¢‘åŒ¹é…)
        """
        from app.services.download_service import DownloadService
        # from app.services.scraper import ScraperService # Avoid circular import if possible, or import inside
        
        logger.info(f"Redownload requested for song {song_id} from {source}:{source_id}")
        
        song_repo = SongRepository(db)
        song = await song_repo.get(song_id)
        if not song:
            logger.error(f"Song {song_id} not found")
            return False
            
        old_path = song.local_path
        
        # 1. Download
        download_service = DownloadService()
        
        # Get URL
        audio_info = await download_service.get_audio_url(source, source_id, quality)
        if not audio_info or not audio_info.get("url"):
            logger.error(f"Failed to get audio url for {source}:{source_id}")
            return False
            
        # Construct new filename from search metadata
        # Priority: audio_info(API result) > Override(from search list) > DB(old record)
        target_title = audio_info.get("title") or title or song.title
        target_artist = audio_info.get("artist") or artist or (song.artist.name if song.artist else "Unknown")
        
        import re
        safe_title = re.sub(r'[<>:"/\\|?*]', '_', target_title)
        safe_artist = re.sub(r'[<>:"/\\|?*]', '_', target_artist)
        
        # Parse extension from URL or fallback
        url = audio_info.get("url", "")
        ext = "mp3"
        if ".flac" in url.lower():
            ext = "flac"
        elif ".wav" in url.lower():
            ext = "wav"
        elif ".m4a" in url.lower():
            ext = "m4a"
        else:
            # Fallback based on bitrate if URL doesn't contain extension
            ext = "flac" if audio_info.get("br", 0) >= 740 else "mp3"
            
        filename = f"{safe_artist} - {safe_title}.{ext}"
        filepath = os.path.join(download_service.cache_dir, filename)
        
        # Download
        success = await download_service.download_file(audio_info["url"], filepath)
        if not success:
            logger.error(f"Failed to download file to {filepath}")
            return False
            
        # 2. Update DB
        logger.info(f"Download successful: {filepath}")
        song.local_path = filepath
        song.status = "DOWNLOADED"
        await db.commit()
        
        # 3. Clean up old file
        if old_path and old_path != filepath:
             if old_path and await anyio.to_thread.run_sync(os.path.exists, old_path):
                 try:
                     await anyio.to_thread.run_sync(os.remove, old_path)
                     logger.info(f"Deleted old file: {old_path}")
                 except Exception as e:
                     logger.warning(f"Failed to delete old file: {e}")

        # 4. Scrape & Apply Metadata (Fix Tags)
        try:
            from app.services.scraper import ScraperService
            from app.services.metadata_service import MetadataService
            from app.services.music_providers import MusicAggregator
            
            scraper = ScraperService(
                aggregator=MusicAggregator(),
                metadata_service=MetadataService()
            )
            
            await scraper.scrape_and_apply(db, song, source, source_id)
            logger.info("Metadata scraped and applied to new file.")
            
        except Exception as e:
            logger.error(f"Failed to apply metadata after redownload: {e}")
            
        return True
