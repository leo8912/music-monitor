# -*- coding: utf-8 -*-
"""
ArtistRefreshService - æ­Œæ‰‹åˆ·æ–°æœåŠ¡

åŠŸèƒ½ï¼š
- ä»åœ¨çº¿æºï¼ˆQQéŸ³ä¹ã€ç½‘æ˜“äº‘ï¼‰åŒæ­¥æ­Œæ‰‹æ­Œæ›²åˆ—è¡¨
- æ™ºèƒ½åˆå¹¶åœ¨çº¿å’Œæœ¬åœ°æ­Œæ›²
- å…ƒæ•°æ®æ²»æ„ˆï¼ˆä¿®å¤å ä½ç¬¦æ—¥æœŸã€ç¼ºå¤±å°é¢ï¼‰
- å­¤å„¿æ­Œæ›²æŒ½æ•‘ï¼ˆå…³è”æœ¬åœ°æ–‡ä»¶åˆ°åœ¨çº¿æºï¼‰

Author: google
Created: 2026-02-02 (ä» LibraryService æ‹†åˆ†)
"""
from typing import List, Dict, Optional
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select
from sqlalchemy.orm import selectinload
from collections import defaultdict
from datetime import datetime
import logging
import re
import json
import uuid

from app.models.artist import Artist, ArtistSource
from app.models.song import Song, SongSource
from app.repositories.artist import ArtistRepository
from app.repositories.song import SongRepository
from app.services.music_providers.aggregator import MusicAggregator
from app.services.scan_service import ScanService
from app.services.metadata_healer import MetadataHealer
from app.utils.error_handler import handle_service_errors

logger = logging.getLogger(__name__)


class ArtistRefreshService:
    """æ­Œæ‰‹åˆ·æ–°æœåŠ¡ - è´Ÿè´£ä»åœ¨çº¿æºåŒæ­¥æ­Œæ›²åˆ—è¡¨"""
    
    def __init__(self):
        self.aggregator = MusicAggregator()
        self.scan_service = ScanService()
        self.metadata_healer = MetadataHealer()
        self._refresh_enrich_count = 0
        self._refresh_heal_count = 0
    
    @handle_service_errors(fallback_value=0)
    async def refresh(self, db: AsyncSession, artist_name: str) -> int:
        """
        å…¨é‡åˆ·æ–°ä¸€åæ­Œæ‰‹çš„æ­Œæ›²èµ„æ–™ã€‚
        """
        logger.info(f"Refreshing artist: {artist_name}")
        
        # 1. è·å–æ­Œæ‰‹ä¿¡æ¯
        artist_repo = ArtistRepository(db)
        artist = await artist_repo.get_by_name(artist_name)
        if not artist:
            logger.warning(f"Artist {artist_name} not found in DB")
            return 0
        
        # 2. é¢„æ‰«ææœ¬åœ°æ–‡ä»¶
        logger.info(f"ğŸ“ [Pre-refresh] Scanning local files for {artist_name}...")
        await self.scan_service.scan_local_files(db)
        
        # 3. å¹¿æ’­å¼€å§‹çŠ¶æ€
        from core.websocket import manager
        await manager.broadcast({
            "type": "artist_progress",
            "artistId": str(artist.id),
            "artistName": artist.name,
            "state": "scanning",
            "progress": 10,
            "message": "ğŸ“¥ æ­£åœ¨æ‹‰å–å…¨ç½‘æ­Œæ›²...",
            "songCount": await artist_repo.get_song_count(artist.id)
        })
        
        # 4. è·å–åœ¨çº¿æ­Œæ›²
        raw_songs = await self._fetch_online_songs(db, artist, manager)
        if not raw_songs:
            return 0
        
        # 5. åå‘æŸ¥æ‰¾ç¼ºå¤±çš„åŸç‰ˆæ­Œæ›²
        raw_songs = await self._reverse_lookup_originals(
            raw_songs, artist, manager
        )
        
        # 6. åˆå¹¶åœ¨çº¿å’Œæœ¬åœ°æ­Œæ›²
        new_count = await self._merge_with_local(
            db, artist, raw_songs, manager
        )
        
        # 7. æŒ½æ•‘å­¤å„¿æ­Œæ›²
        await self._rescue_orphan_songs(db, artist, raw_songs, manager)
        
        # 8. å…¨åº“å…ƒæ•°æ®æ²»æ„ˆ (ä½¿ç”¨ MetadataHealer)
        await self._heal_all_metadata(db, artist)
        
        # 9. å®Œæˆç»Ÿè®¡
        total_count = await artist_repo.get_song_count(artist.id)
        logger.info(
            f"Artist {artist_name} refresh complete. "
            f"Added {new_count} new songs. Total {total_count}."
        )
        
        # 10. å¹¿æ’­å®ŒæˆçŠ¶æ€
        await manager.broadcast({
            "type": "artist_progress",
            "artistId": str(artist.id),
            "artistName": artist.name,
            "state": "complete",
            "progress": 100,
            "message": f"âœ… åˆ·æ–°å®Œæˆ (æ–°å¢ {new_count} é¦–, æ€»è®¡ {total_count} é¦–)",
            "songCount": total_count
        })
        
        # è§¦å‘å‰ç«¯åˆ·æ–°
        await manager.broadcast({
            "type": "refresh_list",
            "artistId": str(artist.id),
            "artistName": artist.name
        })
        
        return new_count
    
    @handle_service_errors(fallback_value=[])
    async def _fetch_online_songs(
        self, 
        db: AsyncSession, 
        artist: Artist,
        manager
    ) -> List:
        """è·å–åœ¨çº¿æ­Œæ›²åˆ—è¡¨"""
        # åŠ è½½æ­Œæ‰‹æº
        stmt = select(ArtistSource).where(ArtistSource.artist_id == artist.id)
        sources = (await db.execute(stmt)).scalars().all()
        artist_ids = {s.source: s.source_id for s in sources}
        
        if not artist_ids:
            logger.info("No source IDs found for artist")
            return []
        
        raw_songs = []
        
        # QQéŸ³ä¹
        if 'qqmusic' in artist_ids:
            await manager.broadcast({
                "type": "artist_progress",
                "artistId": str(artist.id),
                "artistName": artist.name,
                "state": "fetching_qq",
                "progress": 20,
                "message": "ğŸ“¥ æ­£åœ¨æ‹‰å– QQ éŸ³ä¹æ­Œæ›²åˆ—è¡¨..."
            })
            qq_songs = await self.aggregator.providers[1].get_artist_songs(
                artist_ids['qqmusic'], limit=1000
            )
            raw_songs.extend(qq_songs)
            logger.info(f"Fetched {len(qq_songs)} songs from QQ Music")
        
        # ç½‘æ˜“äº‘
        if 'netease' in artist_ids:
            await manager.broadcast({
                "type": "artist_progress",
                "artistId": str(artist.id),
                "artistName": artist.name,
                "state": "fetching_netease",
                "progress": 30,
                "message": "ğŸ“¥ æ­£åœ¨æ‹‰å–ç½‘æ˜“äº‘éŸ³ä¹æ­Œæ›²åˆ—è¡¨..."
            })
            netease_songs = await self.aggregator.providers[0].get_artist_songs(
                artist_ids['netease'], limit=1000
            )
            raw_songs.extend(netease_songs)
            logger.info(f"Fetched {len(netease_songs)} songs from Netease")
        
        # è¿‡æ»¤è„æ•°æ®
        raw_songs = [s for s in raw_songs if self.aggregator._is_valid_song(s)]
        
        # è¡¥å…¨æ­Œæ‰‹å¤´åƒ
        if not artist.avatar:
            for rs in raw_songs:
                if rs.cover:
                    artist.avatar = rs.cover
                    logger.info(f"ğŸ¨ å·²ä»é‡‡é›†åˆ—è¡¨è‡ªåŠ¨è¡¥å…¨è‰ºäººå¤´åƒ: {artist.name}")
                    await db.commit()
                    break
        
        await manager.broadcast({
            "type": "artist_progress",
            "artistId": str(artist.id),
            "artistName": artist.name,
            "state": "matching",
            "progress": 40,
            "message": f"è·å–åˆ° {len(raw_songs)} é¦–æ­Œæ›²ï¼Œæ­£åœ¨èšåˆ..."
        })
        
        return raw_songs
    
    @handle_service_errors(fallback_value=[])
    async def _reverse_lookup_originals(
        self,
        raw_songs: List,
        artist: Artist,
        manager
    ) -> List:
        """åå‘æŸ¥æ‰¾ç¼ºå¤±çš„åŸç‰ˆæ­Œæ›²ï¼ˆé’ˆå¯¹ä¼´å¥ï¼‰"""
        existing_titles_norm = {
            ScanService._normalize_cn_brackets(s.title).lower().strip() 
            for s in raw_songs
        }
        
        extra_songs = []
        checked_inst_titles = set()
        inst_keywords = ['(ä¼´å¥)', '(inst)', 'instrumental', 'ä¼´å¥', 'inst.']
        
        for s in raw_songs:
            title_lower = s.title.lower()
            is_inst = False
            clean_title = s.title
            
            # æ£€æµ‹ä¼´å¥
            for kw in inst_keywords:
                if kw in title_lower:
                    is_inst = True
                    clean_title = s.title.lower().replace(kw, '').strip()
                    clean_title = clean_title.replace('()', '').replace('ï¼ˆï¼‰', '').strip()
                    break
            
            if is_inst:
                norm_clean = ScanService._normalize_cn_brackets(clean_title).lower().strip()
                
                if norm_clean not in existing_titles_norm and norm_clean not in checked_inst_titles:
                    checked_inst_titles.add(norm_clean)
                    logger.info(f"ğŸ” å‘ç°å­¤ç«‹ä¼´å¥ '{s.title}', å°è¯•åæŸ¥åŸç‰ˆ: '{clean_title}'")
                    
                    await manager.broadcast({
                        "type": "artist_progress",
                        "artistId": str(artist.id),
                        "artistName": artist.name,
                        "state": "matching",
                        "progress": 40,
                        "message": f"æ­£åœ¨è¡¥å…¨åŸç‰ˆ: {clean_title}..."
                    })
                    
                    try:
                        query = f"{clean_title} {artist.name}"
                        search_res = await self.aggregator.providers[0].search_song(query, limit=3)
                        
                        found_target = None
                        for cand in search_res:
                            cand_norm = ScanService._normalize_cn_brackets(cand.title).lower().strip()
                            if cand_norm == norm_clean:
                                found_target = cand
                                break
                        
                        if found_target:
                            logger.info(f"  âœ… æˆåŠŸæ‰¾å›åŸç‰ˆ: {found_target.title}")
                            extra_songs.append(found_target)
                            existing_titles_norm.add(norm_clean)
                    except Exception as e:
                        logger.warning(f"  âŒ åæŸ¥å¤±è´¥: {e}")
        
        if extra_songs:
            raw_songs.extend(extra_songs)
            logger.info(f"âœ¨ åå‘è¡¥å…¨äº† {len(extra_songs)} é¦–ç¼ºå¤±çš„åŸç‰ˆæ­Œæ›²")
        
        return raw_songs
    
    async def _merge_with_local(
        self,
        db: AsyncSession,
        artist: Artist,
        raw_songs: List,
        manager
    ) -> int:
        """åˆå¹¶åœ¨çº¿å’Œæœ¬åœ°æ­Œæ›²"""
        await manager.broadcast({
            "type": "artist_progress",
            "artistId": str(artist.id),
            "artistName": artist.name,
            "state": "matching",
            "progress": 50,
            "message": "ğŸ” æ­£åœ¨ä¸æœ¬åœ°åº“åŒ¹é…..."
        })
        
        # æŒ‰æ ‡é¢˜åˆ†ç»„
        grouped_songs = defaultdict(list)
        for s in raw_songs:
            clean_title = ScanService._normalize_cn_brackets(s.title).lower().strip()
            grouped_songs[clean_title].append(s)
        
        # æ’åºï¼ˆæŒ‰å‘å¸ƒæ—¶é—´å€’åºï¼‰
        def get_group_date(group):
            dates = [
                str(getattr(s, 'publish_time', '0000-00-00')) 
                for s in group if getattr(s, 'publish_time', None)
            ]
            return max(dates) if dates else '0000-00-00'
        
        sorted_groups = sorted(
            grouped_songs.items(), 
            key=lambda x: get_group_date(x[1]), 
            reverse=True
        )
        
        # è·å–ç°æœ‰æ­Œæ›²åŠå…¶æ‰€æœ‰çš„æºï¼ˆé€šè¿‡ selectinload é¢„åŠ è½½ï¼Œé¿å… N+1 é—®é¢˜å’Œ MissingGreenlet é”™è¯¯ï¼‰
        res = await db.execute(
            select(Song)
            .options(
                selectinload(Song.sources),
                selectinload(Song.artist)
            )
            .where(Song.artist_id == artist.id)
        )
        all_db_songs = res.scalars().all()
        
        db_song_map = {
            ScanService._normalize_cn_brackets(s.title).lower().strip(): s 
            for s in all_db_songs
        }
        logger.info(f"  ğŸ” å·²ç¼“å­˜ {len(all_db_songs)} é¦–ç°æœ‰æ­Œæ›²ï¼ˆå¸¦æºä¿¡æ¯ï¼‰ç”¨äºæ¨¡ç³ŠåŒ¹é…")
        
        new_count = 0
        processed = 0
        total_groups = len(sorted_groups)
        
        for title_key, group in sorted_groups:
            processed += 1
            
            # é€‰æ‹©æœ€ä½³å…ƒæ•°æ®ï¼ˆä¼˜å…ˆQQéŸ³ä¹ï¼‰
            best_meta = group[0]
            qq_ver = next((x for x in group if x.source == 'qqmusic'), None)
            if qq_ver:
                best_meta = qq_ver
            
            # å™ªå£°è¿‡æ»¤
            noise_keywords = ["#", "å·¡æ¼”", "æœ€åä¸€ç«™", "é¢„å‘Š"]
            if (any(k in best_meta.title for k in noise_keywords) and 
                not best_meta.album and len(best_meta.title) > 30):
                logger.info(f"ğŸ§¹ è¿‡æ»¤å™ªå£°åŠ¨æ€: {best_meta.title}")
                continue
            
            # æŸ¥æ‰¾æˆ–åˆ›å»ºæ­Œæ›²
            norm_key = ScanService._normalize_cn_brackets(best_meta.title).lower().strip()
            existing_song = db_song_map.get(norm_key)
            
            if not existing_song:
                existing_song = Song(
                    artist_id=artist.id,
                    title=best_meta.title,
                    album=best_meta.album,
                    created_at=datetime.now(),
                    status="PENDING",
                    unique_key=str(uuid.uuid4()),
                    sources=[] # æ˜¾å¼åˆå§‹åŒ–ï¼Œé˜²æ­¢ MissingGreenlet
                )
                db.add(existing_song)
                await db.flush()
                new_count += 1
                db_song_map[norm_key] = existing_song
            
            # æ™ºèƒ½åˆå¹¶å…ƒæ•°æ®
            await self._smart_merge_metadata(
                db, existing_song, group, db_song_map, norm_key, artist.name
            )
            
            # è·å–æ­¤æ­Œæ›²çš„æ‰€æœ‰æº - æ­¤å¤„ç”±äºå·² prefetch æˆ–æ˜¾å¼åˆå§‹åŒ–ï¼Œä¸ä¼šæŠ¥é”™
            existing_sources = existing_song.sources
            if not existing_sources:
                pass
            
            src_map = {(src.source, str(src.source_id)) for src in existing_sources}
            
            # æ›´æ–°æº
            for s in group:
                s_id_str = str(s.id)
                if (s.source, s_id_str) not in src_map:
                    # [Fix] Double check against DB to prevent IntegrityError
                    chk = select(SongSource).where(
                        SongSource.song_id == existing_song.id,
                        SongSource.source == s.source,
                        SongSource.source_id == s_id_str
                    )
                    if (await db.execute(chk)).scalars().first():
                        src_map.add((s.source, s_id_str))
                        continue

                    src_ent = SongSource(
                        song_id=existing_song.id,
                        source=s.source,
                        source_id=s_id_str,
                        cover=s.cover_url or getattr(s, 'pic_url', None),
                        duration=s.duration,
                        url=getattr(s, 'url', None),
                        data_json={'quality': getattr(s, 'quality', 'unknown')}
                    )
                    db.add(src_ent)
                    src_map.add((s.source, s_id_str)) # é¿å…åŒä¸€æ‰¹æ¬¡é‡å¤æ·»åŠ 
            
            # è¿›åº¦å¹¿æ’­
            if processed % 20 == 0 or processed == total_groups:
                await manager.broadcast({
                    "type": "artist_progress",
                    "artistId": str(artist.id),
                    "artistName": artist.name,
                    "state": "matching",
                    "progress": int(40 + (processed / total_groups) * 35),
                    "message": f"â³ åŒ¹é…è¿›åº¦ ({processed}/{total_groups})"
                })
        
        await db.commit()
        return new_count
    
    async def _smart_merge_metadata(
        self,
        db: AsyncSession,
        existing_song: Song,
        group: List,
        db_song_map: Dict,
        norm_key: str,
        artist_name: str
    ):
        """æ™ºèƒ½åˆå¹¶å…ƒæ•°æ®"""
        candidate_covers = []
        candidate_dates = []
        candidate_albums = []
        
        for s in group:
            c_url = getattr(s, 'cover_url', None) or getattr(s, 'pic_url', None)
            if c_url:
                candidate_covers.append(c_url)
            
            p_raw = getattr(s, 'publish_time', None)
            if p_raw:
                p_parsed = self.metadata_healer._parse_date(str(p_raw))
                if p_parsed:
                    candidate_dates.append(p_parsed.strftime("%Y-%m-%d"))
            
            alb = getattr(s, 'album', None)
            if alb:
                candidate_albums.append(alb)
        
        # åˆ¤æ–­æ˜¯å¦éœ€è¦æ²»æ„ˆ
        needs_healing = False
        if not existing_song.cover or 'gtimg.cn' in str(existing_song.cover):
            needs_healing = True
        if (not existing_song.publish_time or 
            existing_song.publish_time.year >= 2026 or 
            existing_song.publish_time.year <= 1970):
            needs_healing = True
        if not existing_song.album:
            needs_healing = True
        
        # å…¨ç½‘è¡¥å…¨
        if needs_healing and (not candidate_covers or not candidate_dates):
            if self._refresh_enrich_count < 15:
                logger.info(f"  ğŸ” æ­£åœ¨ä¸º [æ—§æ›²] å°è¯•å…¨ç½‘è¡¥å…¨å…ƒæ•°æ®: {existing_song.title}")
                try:
                    enriched = await self.aggregator.get_song_metadata_from_best_source(
                        existing_song.title, 
                        artist_name
                    )
                    if enriched:
                        if enriched.get('cover_url'):
                            candidate_covers.insert(0, enriched['cover_url'])
                        if enriched.get('publish_time'):
                            p_enrich = self.metadata_healer._parse_date(
                                str(enriched['publish_time'])
                            )
                            if p_enrich:
                                candidate_dates.insert(0, p_enrich.strftime("%Y-%m-%d"))
                        if enriched.get('album') and not candidate_albums:
                            candidate_albums.append(enriched['album'])
                        self._refresh_enrich_count += 1
                except Exception as e:
                    logger.warning(f"Metadata healing failed: {e}")
        
        # åº”ç”¨æ›´æ–°
        if candidate_covers:
            if not existing_song.cover or 'gtimg.cn' in str(existing_song.cover):
                existing_song.cover = candidate_covers[0]
        
        if candidate_albums and not existing_song.album:
            existing_song.album = candidate_albums[0]
        
        # æ›´æ–°æ—¥æœŸ
        new_date = None
        if candidate_dates:
            new_date = self.metadata_healer._parse_date(candidate_dates[0])
        
        # ä¼´å¥ç‰ˆæœ¬å›é€€ç­–ç•¥
        if not new_date and "_inst" in norm_key:
            orig_key = norm_key.replace("_inst", "")
            if orig_key in db_song_map:
                orig_song = db_song_map[orig_key]
                if orig_song.publish_time and 1970 < orig_song.publish_time.year < 2026:
                    new_date = orig_song.publish_time
                    logger.info(
                        f"    ğŸ¹ ä¼´å¥æ—¥æœŸå›é€€: {existing_song.title} -> "
                        f"ç»§æ‰¿åŸç‰ˆ ({new_date.strftime('%Y-%m-%d')})"
                    )
        
        if new_date:
            curr_date = existing_song.publish_time
            if not curr_date or curr_date.year >= 2026 or curr_date.year <= 1970:
                existing_song.publish_time = new_date
                logger.info(
                    f"    ğŸ“… æ—¥æœŸä¿®æ­£: {existing_song.title} -> "
                    f"{new_date.strftime('%Y-%m-%d')}"
                )
            elif abs((curr_date - new_date).days) > 1:
                existing_song.publish_time = new_date
    
    @handle_service_errors(raise_on_critical=False)
    async def _rescue_orphan_songs(
        self,
        db: AsyncSession,
        artist: Artist,
        raw_songs: List,
        manager
    ):
        """æŒ½æ•‘å­¤å„¿æ­Œæ›²ï¼ˆæœ¬åœ°æ–‡ä»¶å…³è”åˆ°åœ¨çº¿æºï¼‰"""
        try:
            stmt = select(Song).options(selectinload(Song.sources)).where(
                Song.artist_id == artist.id,
                Song.local_path != None
            )
            local_songs = (await db.execute(stmt)).scalars().all()
            
            logger.info(f"ğŸš‘ [æŒ½æ•‘æ¨¡å¼] å¼€å§‹æ£€æŸ¥ {len(local_songs)} é¦–æœ¬åœ°æ­Œæ›²...")
            
            await manager.broadcast({
                "type": "artist_progress",
                "artistId": str(artist.id),
                "artistName": artist.name,
                "state": "rescue",
                "progress": 80,
                "message": f"æ­£åœ¨æ£€æŸ¥ {len(local_songs)} é¦–æœ¬åœ°å­¤å„¿æ­Œæ›²..."
            })
            
            rescue_count = 0
            
            for song in local_songs:
                # æ£€æŸ¥æ˜¯å¦å·²æœ‰åœ¨çº¿æº
                has_online = any(s.source in ['qqmusic', 'netease'] for s in song.sources)
                if has_online and song.publish_time:
                    continue
                
                logger.info(f"  ğŸ‘‰ æ­£åœ¨å°è¯•æŒ½æ•‘: {song.title}")
                
                # å°è¯•ä»å†…åµŒæ ‡ç­¾è·å–æ ‡é¢˜
                if song.local_path:
                    try:
                        from tinytag import TinyTag
                        import os
                        if os.path.exists(song.local_path):
                            tag = TinyTag.get(song.local_path)
                            if tag and tag.title:
                                t_clean = ScanService._normalize_cn_brackets(
                                    tag.title
                                ).lower().strip()
                                match_title = ScanService._normalize_cn_brackets(
                                    song.title
                                ).lower().strip()
                                if t_clean and t_clean != match_title:
                                    logger.info(f"    ğŸ·ï¸ ä½¿ç”¨å†…åµŒæ ‡ç­¾æ ‡é¢˜: {tag.title}")
                                    song.title = tag.title
                    except:
                        pass
                
                # æŸ¥æ‰¾åŒ¹é…
                best_match = self._find_match(raw_songs, song)
                
                if not best_match:
                    search_key = f"{song.title} {artist.name}"
                    search_results = await self.aggregator.search_song(search_key, limit=5)
                    best_match = self._find_match(search_results, song)
                
                # å°è¯•å»æ‹¬å·æœç´¢
                if not best_match:
                    clean_title = re.sub(r"[\(\[ã€ï¼ˆ].*?[\)\]ã€‘ï¼‰]", "", song.title).strip()
                    if clean_title and clean_title != song.title:
                        logger.info(f"    âš ï¸ æœªå‘½ä¸­ï¼Œå°è¯•å»æ‹¬å·æœç´¢: '{clean_title}'")
                        clean_key = f"{clean_title} {artist.name}"
                        relaxed_results = await self.aggregator.search_song(clean_key, limit=5)
                        best_match = self._find_match(relaxed_results, song)
                
                if best_match:
                    # æ£€æŸ¥æºæ˜¯å¦å·²å­˜åœ¨
                    chk = select(SongSource).where(
                        SongSource.song_id == song.id,
                        SongSource.source == best_match.source,
                        SongSource.source_id == str(best_match.id)
                    )
                    existing_src = (await db.execute(chk)).scalars().first()
                    
                    if not existing_src:
                        new_source = SongSource(
                            song_id=song.id,
                            source=best_match.source,
                            source_id=best_match.id,
                            cover=best_match.cover_url,
                            duration=best_match.duration,
                            url="",
                            data_json=json.dumps(best_match.__dict__, default=str)
                        )
                        db.add(new_source)
                        logger.info(f"    ğŸ”— å…³è”æˆåŠŸ! æº: {best_match.source}")
                    
                    # è¡¥å…¨å…ƒæ•°æ®
                    if not song.cover and best_match.cover_url:
                        song.cover = best_match.cover_url
                        logger.info(f"    ğŸ–¼ï¸ è¡¥å…¨å°é¢")
                    
                    if not song.album and best_match.album:
                        song.album = best_match.album
                        logger.info(f"    ğŸ’½ è¡¥å…¨ä¸“è¾‘: {song.album}")
                    
                    if not song.publish_time and best_match.publish_time:
                        try:
                            pt_str = str(best_match.publish_time).strip()
                            from datetime import datetime as dt
                            if pt_str.replace('-', '').isdigit() and len(pt_str) >= 10:
                                ts = int(pt_str)
                                if len(pt_str) == 13:
                                    ts = ts / 1000
                                if ts > 0:
                                    song.publish_time = dt.fromtimestamp(ts)
                            elif len(pt_str) == 4 and pt_str.isdigit():
                                song.publish_time = dt.strptime(pt_str, "%Y")
                            elif len(pt_str) >= 10:
                                dt_obj = dt.strptime(pt_str[:10], "%Y-%m-%d")
                                if dt_obj.year > 1970:
                                    song.publish_time = dt_obj
                            if song.publish_time:
                                logger.info(f"    ğŸ“… è¡¥å…¨æ—¥æœŸ: {song.publish_time}")
                        except Exception as e:
                            logger.warning(f"Date parse failed: {e}")
                    
                    rescue_count += 1
            
            if rescue_count > 0:
                await db.commit()
                logger.info(f"âœ¨ æŒ½æ•‘è¡ŒåŠ¨ç»“æŸ: æˆåŠŸä¿®å¤ {rescue_count} é¦–æ­Œæ›²")
        
        except Exception as e:
            logger.error(f"âŒ æŒ½æ•‘æ¨¡å¼å‘ç”Ÿæ„å¤–é”™è¯¯: {e}", exc_info=True)
    
    def _find_match(self, candidates: List, local_song: Song):
        """æŸ¥æ‰¾åŒ¹é…çš„æ­Œæ›²"""
        norm_local = ScanService._normalize_cn_brackets(local_song.title).lower().strip()
        
        variant_keywords = [
            "(ä¼´å¥)", " ä¼´å¥", "inst.", "instrumental", 
            "demo", "(live)", " live", "ï¼ˆä¼´å¥ï¼‰"
        ]
        is_local_variant = any(k in local_song.title.lower() for k in variant_keywords)
        
        for res in candidates:
            norm_res = ScanService._normalize_cn_brackets(res.title).lower().strip()
            is_remote_variant = any(k in res.title.lower() for k in variant_keywords)
            
            # ç²¾ç¡®åŒ¹é…
            if norm_local == norm_res:
                logger.info(f"      -> âœ… ç²¾ç¡®åŒ¹é…æˆåŠŸ: '{res.title}'")
                return res
            
            # æ¨¡ç³ŠåŒ¹é…ï¼ˆRemoteåœ¨Localä¸­ï¼‰
            if norm_res in norm_local:
                if is_local_variant and not is_remote_variant:
                    logger.info(
                        f"      â›” æ‹’ç»æ¨¡ç³ŠåŒ¹é…: æœ¬åœ°æ˜¯å˜ä½“ä½†è¿œç¨‹æ˜¯åŸç‰ˆ"
                    )
                    continue
                logger.info(f"      -> âš ï¸ æ¨¡ç³ŠåŒ¹é…æˆåŠŸ(Remoteåœ¨Localä¸­)")
                return res
            
            # åå‘æ¨¡ç³ŠåŒ¹é…ï¼ˆLocalåœ¨Remoteä¸­ï¼‰
            if len(norm_local) > 1 and norm_local in norm_res:
                if not is_local_variant and is_remote_variant:
                    logger.info(f"      âš ï¸ å…è®¸åå‘æ¨¡ç³ŠåŒ¹é…(å¯èƒ½æ˜¯Liveç‰ˆ)")
                    return res
                logger.info(f"      -> âš ï¸ æ¨¡ç³ŠåŒ¹é…æˆåŠŸ(Localåœ¨Remoteä¸­)")
                return res
        
        return None
    
    @handle_service_errors(raise_on_critical=False)
    async def _heal_all_metadata(self, db: AsyncSession, artist: Artist):
        """
        å¯¹æ­Œæ‰‹çš„æ‰€æœ‰æ­Œæ›²æ‰§è¡Œå…ƒæ•°æ®å¹¶è¡Œæ²»æ„ˆã€‚
        Delegates to MetadataHealer.heal_song.
        """
        logger.info(f"ğŸ¥ å¯åŠ¨æ­Œæ‰‹ [{artist.name}] ä¸“å±å…ƒæ•°æ®æ²»æ„ˆ...")
        
        # Get all songs
        res = await db.execute(
            select(Song).where(Song.artist_id == artist.id)
        )
        all_db_songs = res.scalars().all()
        
        import asyncio
        semaphore = asyncio.Semaphore(5)
        
        async def heal_worker(song):
            async with semaphore:
                # Force=False ensures we respect cooldown if it was just healed
                return await self.metadata_healer.heal_song(song.id, force=False)

        tasks = [heal_worker(s) for s in all_db_songs]
        results = await asyncio.gather(*tasks)
        heal_count = sum(1 for r in results if r)
        
        if heal_count > 0:
            logger.info(f"âœ¨ æ­Œæ‰‹æ²»æ„ˆå®Œæˆ: ä¿®å¤äº† {heal_count} é¦–æ­Œæ›²")
        else:
            logger.info("âœ¨ æ­Œæ‰‹æ²»æ„ˆå®Œæˆ: æ‰€æœ‰æ­Œæ›²çŠ¶æ€è‰¯å¥½")
