# -*- coding: utf-8 -*-
"""
ArtistRefreshService - æ­Œæ‰‹åˆ·æ–°æœåŠ¡

åŠŸèƒ½ï¼š
- ä»Žåœ¨çº¿æºï¼ˆQQéŸ³ä¹ã€ç½‘æ˜“äº‘ï¼‰åŒæ­¥æ­Œæ‰‹æ­Œæ›²åˆ—è¡¨
- æ™ºèƒ½åˆå¹¶åœ¨çº¿å’Œæœ¬åœ°æ­Œæ›²
- å…ƒæ•°æ®æ²»æ„ˆï¼ˆä¿®å¤å ä½ç¬¦æ—¥æœŸã€ç¼ºå¤±å°é¢ï¼‰
- å­¤å„¿æ­Œæ›²æŒ½æ•‘ï¼ˆå…³è”æœ¬åœ°æ–‡ä»¶åˆ°åœ¨çº¿æºï¼‰

Author: google
Created: 2026-02-02 (ä»Ž LibraryService æ‹†åˆ†)
"""
from typing import List, Dict, Optional
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select
from sqlalchemy.orm import selectinload
from collections import defaultdict
from datetime import datetime
import logging
import re
import json

from app.models.artist import Artist, ArtistSource
from app.models.song import Song, SongSource
from app.repositories.artist import ArtistRepository
from app.repositories.song import SongRepository
from app.services.music_providers.aggregator import MusicAggregator
from app.services.scan_service import ScanService
from app.services.enrichment_service import EnrichmentService
from app.utils.error_handler import handle_service_errors

logger = logging.getLogger(__name__)


class ArtistRefreshService:
    """æ­Œæ‰‹åˆ·æ–°æœåŠ¡ - è´Ÿè´£ä»Žåœ¨çº¿æºåŒæ­¥æ­Œæ›²åˆ—è¡¨"""
    
    def __init__(self):
        self.aggregator = MusicAggregator()
        self.scan_service = ScanService()
        self.enrichment_service = EnrichmentService()
        self._refresh_enrich_count = 0
        self._refresh_heal_count = 0
    
    @handle_service_errors(fallback_value=0)
    async def refresh(self, db: AsyncSession, artist_name: str) -> int:
        """
        å…¨é‡åˆ·æ–°ä¸€åæ­Œæ‰‹çš„æ­Œæ›²èµ„æ–™ã€‚
        
        è¯¥æ–¹æ³•æ˜¯æœ¬æœåŠ¡çš„å…¥å£ç‚¹ï¼Œæ‰§è¡Œä»¥ä¸‹æµç¨‹ï¼š
        1. ä»Žæ‰€æœ‰é…ç½®çš„éŸ³ä¹å¹³å°ï¼ˆç½‘æ˜“äº‘ã€QQéŸ³ä¹ï¼‰èŽ·å–æ­Œæ‰‹çš„å…¨é‡æ­Œæ›²åˆ—è¡¨ã€‚
        2. ä¸Žæœ¬åœ°æ•°æ®åº“ä¸­çš„çŽ°æœ‰è®°å½•è¿›è¡Œåˆå¹¶ï¼ˆåŸºäºŽæ ‡é¢˜çš„æ™ºèƒ½åŽ»é‡ï¼‰ã€‚
        3. è¿›è¡Œâ€œåå‘æŸ¥æ‰¾â€ï¼Œå°è¯•ä»Žåœ¨çº¿å¹³å°è¡¥å…¨æœ¬åœ°åº“ä¸­å·²æœ‰çš„åŽŸç‰ˆæ­Œæ›²ã€‚
        4. æŒ½æ•‘â€œå­¤å„¿æ­Œæ›²â€ï¼Œå³æ•°æ®åº“ä¸­å·²å­˜åœ¨ä½†åœ¨çº¿å¹³å°ä¸å†è¿”å›žçš„æ—§ä½œå“ã€‚
        5. å¯¹æ‰€æœ‰è®°å½•è¿›è¡Œå…ƒæ•°æ®â€œæ²»æ„ˆâ€ï¼ˆHealth Check & Repairï¼‰ã€‚
        
        Args:
            db (AsyncSession): å¼‚æ­¥æ•°æ®åº“ä¼šè¯ã€‚
            artist_name (str): æ­Œæ‰‹çš„å®Œæ•´å‡†ç¡®åç§°ã€‚
            
        Returns:
            int: æœ¬æ¬¡åˆ·æ–°æ–°å¢žè¿›åº“çš„æ­Œæ›²æ•°é‡ã€‚
        """
        logger.info(f"Refreshing artist: {artist_name}")
        
        # 1. èŽ·å–æ­Œæ‰‹ä¿¡æ¯
        artist_repo = ArtistRepository(db)
        artist = await artist_repo.get_by_name(artist_name)
        if not artist:
            logger.warning(f"Artist {artist_name} not found in DB")
            return 0
        
        # 2. é¢„æ‰«ææœ¬åœ°æ–‡ä»¶
        logger.info(f"ðŸ“ [Pre-refresh] Scanning local files for {artist_name}...")
        await self.scan_service.scan_local_files(db)
        
        # 3. å¹¿æ’­å¼€å§‹çŠ¶æ€
        from core.websocket import manager
        await manager.broadcast({
            "type": "artist_progress",
            "artistId": str(artist.id),
            "artistName": artist.name,
            "state": "scanning",
            "progress": 10,
            "message": "ðŸ“¥ æ­£åœ¨æ‹‰å–å…¨ç½‘æ­Œæ›²...",
            "songCount": await artist_repo.get_song_count(artist.id)
        })
        
        # 4. èŽ·å–åœ¨çº¿æ­Œæ›²
        raw_songs = await self._fetch_online_songs(db, artist, manager)
        if not raw_songs:
            return 0
        
        # 5. åå‘æŸ¥æ‰¾ç¼ºå¤±çš„åŽŸç‰ˆæ­Œæ›²
        raw_songs = await self._reverse_lookup_originals(
            raw_songs, artist, manager
        )
        
        # 6. åˆå¹¶åœ¨çº¿å’Œæœ¬åœ°æ­Œæ›²
        new_count = await self._merge_with_local(
            db, artist, raw_songs, manager
        )
        
        # 7. æŒ½æ•‘å­¤å„¿æ­Œæ›²
        await self._rescue_orphan_songs(db, artist, raw_songs, manager)
        
        # 8. å…¨åº“å…ƒæ•°æ®æ²»æ„ˆ
        await self._heal_all_metadata(db, artist)
        
        # 9. å®Œæˆç»Ÿè®¡
        total_count = await artist_repo.get_song_count(artist.id)
        logger.info(
            f"Artist {artist_name} refresh complete. "
            f"Added {new_count} new songs. Total {total_count}."
        )
        
        # 10. å¹¿æ’­å®ŒæˆçŠ¶æ€
        await manager.broadcast({
            "type": "artist_progress",
            "artistId": str(artist.id),
            "artistName": artist.name,
            "state": "complete",
            "progress": 100,
            "message": f"âœ… åˆ·æ–°å®Œæˆ (æ–°å¢ž {new_count} é¦–, æ€»è®¡ {total_count} é¦–)",
            "songCount": total_count
        })
        
        # è§¦å‘å‰ç«¯åˆ·æ–°
        await manager.broadcast({
            "type": "refresh_list",
            "artistId": str(artist.id),
            "artistName": artist.name
        })
        
        return new_count
    
    @handle_service_errors(fallback_value=[])
    async def _fetch_online_songs(
        self, 
        db: AsyncSession, 
        artist: Artist,
        manager
    ) -> List:
        """èŽ·å–åœ¨çº¿æ­Œæ›²åˆ—è¡¨"""
        # åŠ è½½æ­Œæ‰‹æº
        stmt = select(ArtistSource).where(ArtistSource.artist_id == artist.id)
        sources = (await db.execute(stmt)).scalars().all()
        artist_ids = {s.source: s.source_id for s in sources}
        
        if not artist_ids:
            logger.info("No source IDs found for artist")
            return []
        
        raw_songs = []
        
        # QQéŸ³ä¹
        if 'qqmusic' in artist_ids:
            await manager.broadcast({
                "type": "artist_progress",
                "artistId": str(artist.id),
                "artistName": artist.name,
                "state": "fetching_qq",
                "progress": 20,
                "message": "ðŸ“¥ æ­£åœ¨æ‹‰å– QQ éŸ³ä¹æ­Œæ›²åˆ—è¡¨..."
            })
            qq_songs = await self.aggregator.providers[1].get_artist_songs(
                artist_ids['qqmusic'], limit=1000
            )
            raw_songs.extend(qq_songs)
            logger.info(f"Fetched {len(qq_songs)} songs from QQ Music")
        
        # ç½‘æ˜“äº‘
        if 'netease' in artist_ids:
            await manager.broadcast({
                "type": "artist_progress",
                "artistId": str(artist.id),
                "artistName": artist.name,
                "state": "fetching_netease",
                "progress": 30,
                "message": "ðŸ“¥ æ­£åœ¨æ‹‰å–ç½‘æ˜“äº‘éŸ³ä¹æ­Œæ›²åˆ—è¡¨..."
            })
            netease_songs = await self.aggregator.providers[0].get_artist_songs(
                artist_ids['netease'], limit=1000
            )
            raw_songs.extend(netease_songs)
            logger.info(f"Fetched {len(netease_songs)} songs from Netease")
        
        # è¿‡æ»¤è„æ•°æ®
        raw_songs = [s for s in raw_songs if self.aggregator._is_valid_song(s)]
        
        # è¡¥å…¨æ­Œæ‰‹å¤´åƒ
        if not artist.avatar:
            for rs in raw_songs:
                if rs.cover:
                    artist.avatar = rs.cover
                    logger.info(f"ðŸŽ¨ å·²ä»Žé‡‡é›†åˆ—è¡¨è‡ªåŠ¨è¡¥å…¨è‰ºäººå¤´åƒ: {artist.name}")
                    await db.commit()
                    break
        
        await manager.broadcast({
            "type": "artist_progress",
            "artistId": str(artist.id),
            "artistName": artist.name,
            "state": "matching",
            "progress": 40,
            "message": f"èŽ·å–åˆ° {len(raw_songs)} é¦–æ­Œæ›²ï¼Œæ­£åœ¨èšåˆ..."
        })
        
        return raw_songs
    
    @handle_service_errors(fallback_value=[])
    async def _reverse_lookup_originals(
        self,
        raw_songs: List,
        artist: Artist,
        manager
    ) -> List:
        """åå‘æŸ¥æ‰¾ç¼ºå¤±çš„åŽŸç‰ˆæ­Œæ›²ï¼ˆé’ˆå¯¹ä¼´å¥ï¼‰"""
        existing_titles_norm = {
            ScanService._normalize_cn_brackets(s.title).lower().strip() 
            for s in raw_songs
        }
        
        extra_songs = []
        checked_inst_titles = set()
        inst_keywords = ['(ä¼´å¥)', '(inst)', 'instrumental', 'ä¼´å¥', 'inst.']
        
        for s in raw_songs:
            title_lower = s.title.lower()
            is_inst = False
            clean_title = s.title
            
            # æ£€æµ‹ä¼´å¥
            for kw in inst_keywords:
                if kw in title_lower:
                    is_inst = True
                    clean_title = s.title.lower().replace(kw, '').strip()
                    clean_title = clean_title.replace('()', '').replace('ï¼ˆï¼‰', '').strip()
                    break
            
            if is_inst:
                norm_clean = ScanService._normalize_cn_brackets(clean_title).lower().strip()
                
                if norm_clean not in existing_titles_norm and norm_clean not in checked_inst_titles:
                    checked_inst_titles.add(norm_clean)
                    logger.info(f"ðŸ” å‘çŽ°å­¤ç«‹ä¼´å¥ '{s.title}', å°è¯•åæŸ¥åŽŸç‰ˆ: '{clean_title}'")
                    
                    await manager.broadcast({
                        "type": "artist_progress",
                        "artistId": str(artist.id),
                        "artistName": artist.name,
                        "state": "matching",
                        "progress": 40,
                        "message": f"æ­£åœ¨è¡¥å…¨åŽŸç‰ˆ: {clean_title}..."
                    })
                    
                    try:
                        query = f"{clean_title} {artist.name}"
                        search_res = await self.aggregator.providers[0].search_song(query, limit=3)
                        
                        found_target = None
                        for cand in search_res:
                            cand_norm = ScanService._normalize_cn_brackets(cand.title).lower().strip()
                            if cand_norm == norm_clean:
                                found_target = cand
                                break
                        
                        if found_target:
                            logger.info(f"  âœ… æˆåŠŸæ‰¾å›žåŽŸç‰ˆ: {found_target.title}")
                            extra_songs.append(found_target)
                            existing_titles_norm.add(norm_clean)
                    except Exception as e:
                        logger.warning(f"  âŒ åæŸ¥å¤±è´¥: {e}")
        
        if extra_songs:
            raw_songs.extend(extra_songs)
            logger.info(f"âœ¨ åå‘è¡¥å…¨äº† {len(extra_songs)} é¦–ç¼ºå¤±çš„åŽŸç‰ˆæ­Œæ›²")
        
        return raw_songs
    
    async def _merge_with_local(
        self,
        db: AsyncSession,
        artist: Artist,
        raw_songs: List,
        manager
    ) -> int:
        """åˆå¹¶åœ¨çº¿å’Œæœ¬åœ°æ­Œæ›²"""
        await manager.broadcast({
            "type": "artist_progress",
            "artistId": str(artist.id),
            "artistName": artist.name,
            "state": "matching",
            "progress": 50,
            "message": "ðŸ” æ­£åœ¨ä¸Žæœ¬åœ°åº“åŒ¹é…..."
        })
        
        # æŒ‰æ ‡é¢˜åˆ†ç»„
        grouped_songs = defaultdict(list)
        for s in raw_songs:
            clean_title = ScanService._normalize_cn_brackets(s.title).lower().strip()
            grouped_songs[clean_title].append(s)
        
        # æŽ’åºï¼ˆæŒ‰å‘å¸ƒæ—¶é—´å€’åºï¼‰
        def get_group_date(group):
            dates = [
                str(getattr(s, 'publish_time', '0000-00-00')) 
                for s in group if getattr(s, 'publish_time', None)
            ]
            return max(dates) if dates else '0000-00-00'
        
        sorted_groups = sorted(
            grouped_songs.items(), 
            key=lambda x: get_group_date(x[1]), 
            reverse=True
        )
        
        # èŽ·å–çŽ°æœ‰æ­Œæ›²åŠå…¶æ‰€æœ‰çš„æºï¼ˆé€šè¿‡ selectinload é¢„åŠ è½½ï¼Œé¿å… N+1 é—®é¢˜ï¼‰
        res = await db.execute(
            select(Song)
            .options(selectinload(Song.sources))
            .where(Song.artist_id == artist.id)
        )
        all_db_songs = res.scalars().all()
        
        db_song_map = {
            ScanService._normalize_cn_brackets(s.title).lower().strip(): s 
            for s in all_db_songs
        }
        logger.info(f"  ðŸ” å·²ç¼“å­˜ {len(all_db_songs)} é¦–çŽ°æœ‰æ­Œæ›²ï¼ˆå¸¦æºä¿¡æ¯ï¼‰ç”¨äºŽæ¨¡ç³ŠåŒ¹é…")
        
        new_count = 0
        processed = 0
        total_groups = len(sorted_groups)
        
        for title_key, group in sorted_groups:
            processed += 1
            
            # é€‰æ‹©æœ€ä½³å…ƒæ•°æ®ï¼ˆä¼˜å…ˆQQéŸ³ä¹ï¼‰
            best_meta = group[0]
            qq_ver = next((x for x in group if x.source == 'qqmusic'), None)
            if qq_ver:
                best_meta = qq_ver
            
            # å™ªå£°è¿‡æ»¤
            noise_keywords = ["#", "å·¡æ¼”", "æœ€åŽä¸€ç«™", "é¢„å‘Š"]
            if (any(k in best_meta.title for k in noise_keywords) and 
                not best_meta.album and len(best_meta.title) > 30):
                logger.info(f"ðŸ§¹ è¿‡æ»¤å™ªå£°åŠ¨æ€: {best_meta.title}")
                continue
            
            # æŸ¥æ‰¾æˆ–åˆ›å»ºæ­Œæ›²
            norm_key = ScanService._normalize_cn_brackets(best_meta.title).lower().strip()
            existing_song = db_song_map.get(norm_key)
            
            if not existing_song:
                existing_song = Song(
                    artist_id=artist.id,
                    title=best_meta.title,
                    album=best_meta.album,
                    created_at=datetime.now(),
                    status="PENDING"
                )
                db.add(existing_song)
                await db.flush()
                new_count += 1
                db_song_map[norm_key] = existing_song
            
            # æ™ºèƒ½åˆå¹¶å…ƒæ•°æ®
            await self._smart_merge_metadata(
                db, existing_song, group, db_song_map, norm_key
            )
            
            # èŽ·å–æ­¤æ­Œæ›²çš„æ‰€æœ‰æº
            existing_sources = getattr(existing_song, 'sources', [])
            if not existing_sources:
                # å¦‚æžœæ²¡æœ‰åŠ è½½å…³ç³»ï¼Œåˆ™æ‰‹åŠ¨æŸ¥ä¸€ä¸‹ï¼ˆæˆ–ç¡®ä¿ä¹‹å‰ prefetch äº†ï¼‰
                # è¿™é‡Œä¸ºäº†ç®€å•ï¼Œå…ˆç”¨ map
                pass
            
            src_map = {(src.source, str(src.source_id)) for src in existing_sources}
            
            # æ›´æ–°æº
            for s in group:
                s_id_str = str(s.id)
                if (s.source, s_id_str) not in src_map:
                    src_ent = SongSource(
                        song_id=existing_song.id,
                        source=s.source,
                        source_id=s_id_str,
                        cover=s.cover_url or getattr(s, 'pic_url', None),
                        duration=s.duration,
                        url=getattr(s, 'url', None),
                        data_json={'quality': getattr(s, 'quality', 'unknown')}
                    )
                    db.add(src_ent)
                    src_map.add((s.source, s_id_str)) # é¿å…åŒä¸€æ‰¹æ¬¡é‡å¤æ·»åŠ 
            
            # è¿›åº¦å¹¿æ’­
            if processed % 20 == 0 or processed == total_groups:
                await manager.broadcast({
                    "type": "artist_progress",
                    "artistId": str(artist.id),
                    "artistName": artist.name,
                    "state": "matching",
                    "progress": int(40 + (processed / total_groups) * 35),
                    "message": f"â³ åŒ¹é…è¿›åº¦ ({processed}/{total_groups})"
                })
        
        await db.commit()
        return new_count
    
    async def _smart_merge_metadata(
        self,
        db: AsyncSession,
        existing_song: Song,
        group: List,
        db_song_map: Dict,
        norm_key: str
    ):
        """æ™ºèƒ½åˆå¹¶å…ƒæ•°æ®"""
        candidate_covers = []
        candidate_dates = []
        candidate_albums = []
        
        for s in group:
            c_url = getattr(s, 'cover_url', None) or getattr(s, 'pic_url', None)
            if c_url:
                candidate_covers.append(c_url)
            
            p_raw = getattr(s, 'publish_time', None)
            if p_raw:
                p_parsed = self.enrichment_service._parse_date(str(p_raw))
                if p_parsed:
                    candidate_dates.append(p_parsed.strftime("%Y-%m-%d"))
            
            alb = getattr(s, 'album', None)
            if alb:
                candidate_albums.append(alb)
        
        # åˆ¤æ–­æ˜¯å¦éœ€è¦æ²»æ„ˆ
        needs_healing = False
        if not existing_song.cover or 'gtimg.cn' in str(existing_song.cover):
            needs_healing = True
        if (not existing_song.publish_time or 
            existing_song.publish_time.year >= 2026 or 
            existing_song.publish_time.year <= 1970):
            needs_healing = True
        if not existing_song.album:
            needs_healing = True
        
        # å…¨ç½‘è¡¥å…¨
        if needs_healing and (not candidate_covers or not candidate_dates):
            if self._refresh_enrich_count < 15:
                logger.info(f"  ðŸ” æ­£åœ¨ä¸º [æ—§æ›²] å°è¯•å…¨ç½‘è¡¥å…¨å…ƒæ•°æ®: {existing_song.title}")
                try:
                    enriched = await self.aggregator.get_song_metadata_from_best_source(
                        existing_song.title, 
                        existing_song.artist.name
                    )
                    if enriched:
                        if enriched.get('cover_url'):
                            candidate_covers.insert(0, enriched['cover_url'])
                        if enriched.get('publish_time'):
                            p_enrich = self.enrichment_service._parse_date(
                                str(enriched['publish_time'])
                            )
                            if p_enrich:
                                candidate_dates.insert(0, p_enrich.strftime("%Y-%m-%d"))
                        if enriched.get('album') and not candidate_albums:
                            candidate_albums.append(enriched['album'])
                        self._refresh_enrich_count += 1
                except Exception as e:
                    logger.warning(f"Metadata healing failed: {e}")
        
        # åº”ç”¨æ›´æ–°
        if candidate_covers:
            if not existing_song.cover or 'gtimg.cn' in str(existing_song.cover):
                existing_song.cover = candidate_covers[0]
        
        if candidate_albums and not existing_song.album:
            existing_song.album = candidate_albums[0]
        
        # æ›´æ–°æ—¥æœŸ
        new_date = None
        if candidate_dates:
            new_date = self.enrichment_service._parse_date(candidate_dates[0])
        
        # ä¼´å¥ç‰ˆæœ¬å›žé€€ç­–ç•¥
        if not new_date and "_inst" in norm_key:
            orig_key = norm_key.replace("_inst", "")
            if orig_key in db_song_map:
                orig_song = db_song_map[orig_key]
                if orig_song.publish_time and 1970 < orig_song.publish_time.year < 2026:
                    new_date = orig_song.publish_time
                    logger.info(
                        f"    ðŸŽ¹ ä¼´å¥æ—¥æœŸå›žé€€: {existing_song.title} -> "
                        f"ç»§æ‰¿åŽŸç‰ˆ ({new_date.strftime('%Y-%m-%d')})"
                    )
        
        if new_date:
            curr_date = existing_song.publish_time
            if not curr_date or curr_date.year >= 2026 or curr_date.year <= 1970:
                existing_song.publish_time = new_date
                logger.info(
                    f"    ðŸ“… æ—¥æœŸä¿®æ­£: {existing_song.title} -> "
                    f"{new_date.strftime('%Y-%m-%d')}"
                )
            elif abs((curr_date - new_date).days) > 1:
                existing_song.publish_time = new_date
    
    @handle_service_errors(raise_on_critical=False)
    async def _rescue_orphan_songs(
        self,
        db: AsyncSession,
        artist: Artist,
        raw_songs: List,
        manager
    ):
        """æŒ½æ•‘å­¤å„¿æ­Œæ›²ï¼ˆæœ¬åœ°æ–‡ä»¶å…³è”åˆ°åœ¨çº¿æºï¼‰"""
        try:
            stmt = select(Song).options(selectinload(Song.sources)).where(
                Song.artist_id == artist.id,
                Song.local_path != None
            )
            local_songs = (await db.execute(stmt)).scalars().all()
            
            logger.info(f"ðŸš‘ [æŒ½æ•‘æ¨¡å¼] å¼€å§‹æ£€æŸ¥ {len(local_songs)} é¦–æœ¬åœ°æ­Œæ›²...")
            
            await manager.broadcast({
                "type": "artist_progress",
                "artistId": str(artist.id),
                "artistName": artist.name,
                "state": "rescue",
                "progress": 80,
                "message": f"æ­£åœ¨æ£€æŸ¥ {len(local_songs)} é¦–æœ¬åœ°å­¤å„¿æ­Œæ›²..."
            })
            
            rescue_count = 0
            
            for song in local_songs:
                # æ£€æŸ¥æ˜¯å¦å·²æœ‰åœ¨çº¿æº
                has_online = any(s.source in ['qqmusic', 'netease'] for s in song.sources)
                if has_online and song.publish_time:
                    continue
                
                logger.info(f"  ðŸ‘‰ æ­£åœ¨å°è¯•æŒ½æ•‘: {song.title}")
                
                # å°è¯•ä»Žå†…åµŒæ ‡ç­¾èŽ·å–æ ‡é¢˜
                if song.local_path:
                    try:
                        from tinytag import TinyTag
                        import os
                        if os.path.exists(song.local_path):
                            tag = TinyTag.get(song.local_path)
                            if tag and tag.title:
                                t_clean = ScanService._normalize_cn_brackets(
                                    tag.title
                                ).lower().strip()
                                match_title = ScanService._normalize_cn_brackets(
                                    song.title
                                ).lower().strip()
                                if t_clean and t_clean != match_title:
                                    logger.info(f"    ðŸ·ï¸ ä½¿ç”¨å†…åµŒæ ‡ç­¾æ ‡é¢˜: {tag.title}")
                                    song.title = tag.title
                    except:
                        pass
                
                # æŸ¥æ‰¾åŒ¹é…
                best_match = self._find_match(raw_songs, song)
                
                if not best_match:
                    search_key = f"{song.title} {artist.name}"
                    search_results = await self.aggregator.search_song(search_key, limit=5)
                    best_match = self._find_match(search_results, song)
                
                # å°è¯•åŽ»æ‹¬å·æœç´¢
                if not best_match:
                    clean_title = re.sub(r"[\(\[ã€ï¼ˆ].*?[\)\]ã€‘ï¼‰]", "", song.title).strip()
                    if clean_title and clean_title != song.title:
                        logger.info(f"    âš ï¸ æœªå‘½ä¸­ï¼Œå°è¯•åŽ»æ‹¬å·æœç´¢: '{clean_title}'")
                        clean_key = f"{clean_title} {artist.name}"
                        relaxed_results = await self.aggregator.search_song(clean_key, limit=5)
                        best_match = self._find_match(relaxed_results, song)
                
                if best_match:
                    # æ£€æŸ¥æºæ˜¯å¦å·²å­˜åœ¨
                    chk = select(SongSource).where(
                        SongSource.song_id == song.id,
                        SongSource.source == best_match.source,
                        SongSource.source_id == str(best_match.id)
                    )
                    existing_src = (await db.execute(chk)).scalars().first()
                    
                    if not existing_src:
                        new_source = SongSource(
                            song_id=song.id,
                            source=best_match.source,
                            source_id=best_match.id,
                            cover=best_match.cover_url,
                            duration=best_match.duration,
                            url="",
                            data_json=json.dumps(best_match.__dict__, default=str)
                        )
                        db.add(new_source)
                        logger.info(f"    ðŸ”— å…³è”æˆåŠŸ! æº: {best_match.source}")
                    
                    # è¡¥å…¨å…ƒæ•°æ®
                    if not song.cover and best_match.cover_url:
                        song.cover = best_match.cover_url
                        logger.info(f"    ðŸ–¼ï¸ è¡¥å…¨å°é¢")
                    
                    if not song.album and best_match.album:
                        song.album = best_match.album
                        logger.info(f"    ðŸ’½ è¡¥å…¨ä¸“è¾‘: {song.album}")
                    
                    if not song.publish_time and best_match.publish_time:
                        try:
                            pt_str = str(best_match.publish_time).strip()
                            from datetime import datetime as dt
                            if pt_str.replace('-', '').isdigit() and len(pt_str) >= 10:
                                ts = int(pt_str)
                                if len(pt_str) == 13:
                                    ts = ts / 1000
                                if ts > 0:
                                    song.publish_time = dt.fromtimestamp(ts)
                            elif len(pt_str) == 4 and pt_str.isdigit():
                                song.publish_time = dt.strptime(pt_str, "%Y")
                            elif len(pt_str) >= 10:
                                dt_obj = dt.strptime(pt_str[:10], "%Y-%m-%d")
                                if dt_obj.year > 1970:
                                    song.publish_time = dt_obj
                            if song.publish_time:
                                logger.info(f"    ðŸ“… è¡¥å…¨æ—¥æœŸ: {song.publish_time}")
                        except Exception as e:
                            logger.warning(f"Date parse failed: {e}")
                    
                    rescue_count += 1
            
            if rescue_count > 0:
                await db.commit()
                logger.info(f"âœ¨ æŒ½æ•‘è¡ŒåŠ¨ç»“æŸ: æˆåŠŸä¿®å¤ {rescue_count} é¦–æ­Œæ›²")
        
        except Exception as e:
            logger.error(f"âŒ æŒ½æ•‘æ¨¡å¼å‘ç”Ÿæ„å¤–é”™è¯¯: {e}", exc_info=True)
    
    def _find_match(self, candidates: List, local_song: Song):
        """æŸ¥æ‰¾åŒ¹é…çš„æ­Œæ›²"""
        norm_local = ScanService._normalize_cn_brackets(local_song.title).lower().strip()
        
        variant_keywords = [
            "(ä¼´å¥)", " ä¼´å¥", "inst.", "instrumental", 
            "demo", "(live)", " live", "ï¼ˆä¼´å¥ï¼‰"
        ]
        is_local_variant = any(k in local_song.title.lower() for k in variant_keywords)
        
        for res in candidates:
            norm_res = ScanService._normalize_cn_brackets(res.title).lower().strip()
            is_remote_variant = any(k in res.title.lower() for k in variant_keywords)
            
            # ç²¾ç¡®åŒ¹é…
            if norm_local == norm_res:
                logger.info(f"      -> âœ… ç²¾ç¡®åŒ¹é…æˆåŠŸ: '{res.title}'")
                return res
            
            # æ¨¡ç³ŠåŒ¹é…ï¼ˆRemoteåœ¨Localä¸­ï¼‰
            if norm_res in norm_local:
                if is_local_variant and not is_remote_variant:
                    logger.info(
                        f"      â›” æ‹’ç»æ¨¡ç³ŠåŒ¹é…: æœ¬åœ°æ˜¯å˜ä½“ä½†è¿œç¨‹æ˜¯åŽŸç‰ˆ"
                    )
                    continue
                logger.info(f"      -> âš ï¸ æ¨¡ç³ŠåŒ¹é…æˆåŠŸ(Remoteåœ¨Localä¸­)")
                return res
            
            # åå‘æ¨¡ç³ŠåŒ¹é…ï¼ˆLocalåœ¨Remoteä¸­ï¼‰
            if len(norm_local) > 1 and norm_local in norm_res:
                if not is_local_variant and is_remote_variant:
                    logger.info(f"      âš ï¸ å…è®¸åå‘æ¨¡ç³ŠåŒ¹é…(å¯èƒ½æ˜¯Liveç‰ˆ)")
                    return res
                logger.info(f"      -> âš ï¸ æ¨¡ç³ŠåŒ¹é…æˆåŠŸ(Localåœ¨Remoteä¸­)")
                return res
        
        return None
    
    @handle_service_errors(raise_on_critical=False)
    async def _heal_all_metadata(self, db: AsyncSession, artist: Artist):
        """
        å¯¹æ­Œæ‰‹çš„æ‰€æœ‰æ­Œæ›²æ‰§è¡Œå…ƒæ•°æ®å¹¶è¡Œæ²»æ„ˆã€‚
        
        é€šè¿‡å¹¶å‘ï¼ˆå—é™ï¼‰è¯·æ±‚å„ä¸ªå¹³å°ï¼Œå°è¯•è‡ªåŠ¨ä¿®å¤ä»¥ä¸‹é—®é¢˜ï¼š
        - ç¼ºå¤±çš„é«˜æ¸…å°é¢å›¾ã€‚
        - ç¼ºå¤±æˆ–æ ¼å¼ä¸è§„èŒƒçš„ä¸“è¾‘åç§°ã€‚
        - ç¼ºå¤±æˆ–æ˜Žæ˜¾çš„å ä½ç¬¦å‘å¸ƒæ—¥æœŸï¼ˆå¦‚ 1970-01-01ï¼‰ã€‚
        
        ä¼˜åŒ–ç‰¹æ€§:
        - ä½¿ç”¨ asyncio.gather å®žçŽ°å¹¶è¡Œå¤„ç†ã€‚
        - ä½¿ç”¨ asyncio.Semaphore(5) é™åˆ¶å¹¶å‘æ•°ï¼Œé˜²æ­¢è§¦å‘ API é£Žæš´æˆ–è¢«å°ç¦ã€‚
        - å¢žé‡æäº¤ï¼šä»…åœ¨å®žé™…æœ‰æ›´æ–°æ—¶æ“ä½œæ•°æ®åº“ã€‚
        
        Args:
            db (AsyncSession): å¼‚æ­¥æ•°æ®åº“ä¼šè¯ã€‚
            artist (Artist): ç›®æ ‡æ­Œæ‰‹çš„æ¨¡åž‹å¯¹è±¡ã€‚
        """
        logger.info("ðŸ¥ å¯åŠ¨å…¨åº“å…ƒæ•°æ®æ²»æ„ˆ...")
        
        # èŽ·å–æ‰€æœ‰æ­Œæ›²
        res = await db.execute(select(Song).where(Song.artist_id == artist.id))
        all_db_songs = res.scalars().all()
        
        # å»ºç«‹æ ‡é¢˜æ˜ å°„ï¼ˆç”¨äºŽä¼´å¥å›žé€€ï¼‰
        title_map = {s.title: s for s in all_db_songs}
        
        import asyncio
        semaphore = asyncio.Semaphore(5)  # é™åˆ¶å¹¶å‘æ•°ä¸º 5ï¼Œé˜²æ­¢è¢« API å°ç¦
        
        async def heal_with_semaphore(song):
            async with semaphore:
                return await self._heal_single_song(db, song, artist, title_map)
        
        # åˆ›å»ºä»»åŠ¡åˆ—è¡¨
        tasks = [heal_with_semaphore(s) for s in all_db_songs]
        
        # æ‰¹é‡æ‰§è¡Œ
        results = await asyncio.gather(*tasks)
        heal_count = sum(1 for r in results if r)
        
        if heal_count > 0:
            await db.commit()
            logger.info(f"âœ¨ å…¨åº“æ²»æ„ˆå®Œæˆ: å¹¶è¡Œä¿®å¤äº† {heal_count} é¦–æ­Œæ›²çš„å…ƒæ•°æ®")
        else:
            logger.info("âœ¨ å…¨åº“æ²»æ„ˆå®Œæˆ: æ‰€æœ‰æ­Œæ›²çŠ¶æ€è‰¯å¥½")

    async def _heal_single_song(self, db: AsyncSession, s: Song, artist: Artist, title_map: Dict) -> bool:
        """
        æ²»æ„ˆå•é¦–æ­Œæ›²çš„å…ƒæ•°æ®ã€‚ï¼ˆä¾› _heal_all_metadata è°ƒç”¨ï¼‰
        
        Args:
            db (AsyncSession): æ•°æ®åº“ä¼šè¯ã€‚
            s (Song): å¾…æ£€æŸ¥çš„æ­Œæ›²å¯¹è±¡ã€‚
            artist (Artist): æ‰€å±žæ­Œæ‰‹ã€‚
            title_map (Dict): æ­Œæ‰‹æ‰€æœ‰æ­Œæ›²çš„æ ‡é¢˜åˆ°æ­Œæ›²å¯¹è±¡çš„æ˜ å°„ï¼Œç”¨äºŽä¼´å¥å›žé€€ã€‚
            
        Returns:
            bool: å¦‚æžœè¯¥æ­Œæ›²æœ‰ä»»ä½•å­—æ®µè¢«æ›´æ–°ï¼Œè¿”å›ž Trueã€‚
        """
        needs_update = False
        
        # æ£€æŸ¥æ—¥æœŸæœ‰æ•ˆæ€§
        is_invalid_date = False
        if not s.publish_time:
            is_invalid_date = True
        else:
            y = s.publish_time.year
            if y > datetime.now().year + 1 or y <= 1970:
                is_invalid_date = True
        
        # æ£€æŸ¥å°é¢
        is_missing_cover = False
        if not s.cover:
            is_missing_cover = True
        else:
            cover_str = str(s.cover)
            if 'placeholder' in cover_str or 'T002R300x300M000.jpg' in cover_str:
                is_missing_cover = True
        
        # ä¼´å¥ç‰ˆå›žé€€ç­–ç•¥
        if is_invalid_date and ("ä¼´å¥" in s.title or "Inst" in s.title):
            orig_title = re.sub(
                r"[\(\[ã€ï¼ˆ].*?(ä¼´å¥|Inst|Backing).*?[\)\]ã€‘ï¼‰]", 
                "", 
                s.title, 
                flags=re.IGNORECASE
            ).strip()
            if orig_title in title_map:
                orig = title_map[orig_title]
                if (orig.publish_time and 
                    1970 < orig.publish_time.year <= datetime.now().year + 1):
                    s.publish_time = orig.publish_time
                    needs_update = True
                    is_invalid_date = False
        
        # å…¨ç½‘è¡¥å…¨
        if is_invalid_date or is_missing_cover:
            # TODO: è¿™é‡Œéœ€è¦ä¸€ä¸ªè·¨è¯·æ±‚çš„è®¡æ•°å™¨æˆ–é™åˆ¶
            # æš‚æ—¶ä¿æŒçŽ°æœ‰é€»è¾‘ï¼Œä½†è¦æ³¨æ„å¹¶å‘æ—¶çš„è®¡æ•°
            if self._refresh_heal_count < 100:
                try:
                    meta = await self.aggregator.get_song_metadata_from_best_source(
                        s.title, artist.name
                    )
                    if meta:
                        if is_invalid_date and meta.get('publish_time'):
                            p_parsed = self.enrichment_service._parse_date(str(meta['publish_time']))
                            if p_parsed:
                                s.publish_time = p_parsed
                                needs_update = True
                        
                        if is_missing_cover and meta.get('cover_url'):
                            s.cover = meta['cover_url']
                            needs_update = True
                        
                        if not s.album and meta.get('album'):
                            s.album = meta['album']
                            needs_update = True
                        
                        self._refresh_heal_count += 1
                except Exception as e:
                    pass # è¿™é‡Œçš„å¼‚å¸¸ç”± gather å¤„ç†æˆ–æŠ‘åˆ¶
        
        return needs_update
